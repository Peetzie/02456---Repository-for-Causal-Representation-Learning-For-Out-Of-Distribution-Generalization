{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from sklearn import metrics\n",
    "\n",
    "from typing import *\n",
    "from IPython.display import Image, display, clear_output\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import math \n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus, relu\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import Bernoulli, Normal\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "\n",
    "from plotting import make_vae_plots\n",
    "\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "transform_to_tensor = ToTensor()\n",
    "#transform_to_tensor(images[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Bernoulli(logits=torch.zeros((1000,)))\n",
    "\n",
    "\n",
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        eps = torch.empty_like(self.mu).normal_()\n",
    "        return self.mu + self.sigma * eps\n",
    "    \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        return - ((z - self.mu)**2)/(2*self.sigma**2) - torch.log(self.sigma) - math.log(math.sqrt(2 * math.pi)) # <- your code\n",
    "    \n",
    "    def mu(self):\n",
    "        return(self.mu, self.sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.distributions import Distribution\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthNN(nn.Module):\n",
    "    def __init__(self, hidden_size = 6):\n",
    "        super().__init__()\n",
    "        self.function = nn.Sequential(\n",
    "            nn.Linear(in_features=3, out_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=10),\n",
    "        )\n",
    "    def forward(self, Z):\n",
    "        return self.function(Z)\n",
    "\n",
    "\n",
    "def simulate_synthetic_data(samples, function):\n",
    "    envs1 = np.array([0.2, 2, 3, 5])\n",
    "    E = np.random.choice([0,1,2,3], size = samples)\n",
    "\n",
    "    env = envs1[E]\n",
    "    Z1 = env + np.random.normal(0, 1, size = samples)\n",
    "    Z2 = 2*env + np.random.normal(0, np.sqrt(2), size = samples)\n",
    "    Y = Z1 + Z2 + np.random.normal(0, 1, size = samples)\n",
    "    Z3 = Y + np.random.normal(0, 1, size = samples)\n",
    "    Z = np.stack([Z1, Z2, Z3], axis = 1)\n",
    "\n",
    "    if function == 'identity':\n",
    "        X = Z\n",
    "    elif function == 'linear':\n",
    "        S = np.random.normal(size = (2,10))\n",
    "        X = Z@S\n",
    "    elif function == 'nonlinear':\n",
    "        synthnn = SynthNN()\n",
    "        X = synthnn(torch.tensor(Z).float()).detach().numpy()\n",
    "    \n",
    "    return X, Y, E, Z, env\n",
    "\n",
    "class EnvDataset(Dataset):\n",
    "    def __init__(self, X, Y, E):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X)\n",
    "        self.Y = torch.tensor(Y).unsqueeze(1)\n",
    "        E = torch.tensor(E) #From Peter\n",
    "        # self.E = torch.nn.functional.one_hot(torch.tensor(E))\n",
    "        self.E = torch.nn.functional.one_hot(E.long())\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index], self.E[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.X))\n",
    "        \n",
    "model = SynthNN()\n",
    "#x, y, e, z, env = simulate_synthetic_data(100, \"nonlinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "torch.manual_seed(1)\n",
    "X, Y, envs, Z, E = simulate_synthetic_data(20000, 'nonlinear')\n",
    "dset1 = EnvDataset(X[:15000],Y[:15000],envs[:15000])\n",
    "train_loader = DataLoader(dset1, batch_size=128)\n",
    "\n",
    "\n",
    "dset2 = EnvDataset(X[5000:],Y[5000:],envs[5000:])\n",
    "test_loader = DataLoader(dset2, batch_size=128)\n",
    "\n",
    "#test = next(iter(train_loader))\n",
    "#print(test[2])\n",
    "\n",
    "\n",
    "# Load a batch of images into memory\n",
    "sample = next(iter(train_loader))\n",
    "images = sample[0]\n",
    "label = sample[1]\n",
    "environment = sample[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(\n",
      "  (Lambdaflinear_prior): Sequential(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=4, bias=True)\n",
      "  )\n",
      "  (Lambdafnl_prior): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=6, out_features=6, bias=True)\n",
      "  )\n",
      "  (TNN_prior): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=6, out_features=3, bias=True)\n",
      "  )\n",
      "  (LambdaNN): Sequential(\n",
      "    (0): Linear(in_features=5, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=6, out_features=1, bias=True)\n",
      "  )\n",
      "  (LinearEncoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=3, bias=True)\n",
      "  )\n",
      "  (NonLinearEncoder): Sequential(\n",
      "    (0): Linear(in_features=15, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=6, out_features=6, bias=True)\n",
      "  )\n",
      "  (LinearDecoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      "  (NonLinearDecoder): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (1): Linear(in_features=128, out_features=6, bias=True)\n",
      "    (2): ReLU()\n",
      "    (3): Linear(in_features=6, out_features=10, bias=True)\n",
      "  )\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.beta = 1\n",
    "\n",
    "        \n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        ## setting the prior to a vector consisting of zeros with dimensions (1,2*latent_features)\n",
    "        # self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        \n",
    "        '''\n",
    "        According to page 31-32 the iVAE consist of 7 NNs:\n",
    "        1. TNN prior\n",
    "        2. lambdaNN prior\n",
    "        3. lambdaf prior\n",
    "        \n",
    "        4. X-encoder (Classic image CNN)\n",
    "        5. (Y, E)-encoder\n",
    "        6. (X, Y, E)-merger/encoder\n",
    "\n",
    "        7. Decoder\n",
    "\n",
    "        1-3: Learn priors based on the label distribution for the given environment\n",
    "        4-6: Encoding X, encoding Y and E and merging these two encoders, to generate a \n",
    "             qz which is conditional on the environment.\n",
    "        7: Decodes the latent space through pz. Since the latent space now contain some measure\n",
    "           of environment, then this distribution pz is consequentially conditioned on the environment\n",
    "\n",
    "        NN 1-3 can be found in the variational inference funktion.\n",
    "        '''\n",
    "        ####\n",
    "        #1/8\n",
    "        self.Lambdaflinear_prior = nn.Sequential(\n",
    "            nn.Linear(in_features = 2, out_features = 50),\n",
    "            nn.Linear(in_features = 50, out_features = 4)\n",
    "        )\n",
    "\n",
    "\n",
    "        #2/8\n",
    "        self.Lambdafnl_prior = nn.Sequential(\n",
    "            nn.Linear(in_features = 5, out_features = 128),\n",
    "            nn.Linear(in_features = 128, out_features = 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 6, out_features = 6)\n",
    "        )\n",
    "\n",
    "        #3/8\n",
    "        self.TNN_prior = nn.Sequential(\n",
    "            nn.Linear(in_features = 3, out_features = 128),\n",
    "            nn.Linear(in_features = 128, out_features = 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 6, out_features = 3)\n",
    "        )\n",
    "\n",
    "        #4/8\n",
    "        self.LambdaNN = nn.Sequential(\n",
    "            nn.Linear(in_features = 5, out_features = 128),\n",
    "            nn.Linear(in_features = 128, out_features = 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 6, out_features = 1)\n",
    "        )\n",
    "\n",
    "        #5/8\n",
    "        self.LinearEncoder = nn.Sequential(\n",
    "            nn.Linear(in_features = latent_features, out_features = 50),\n",
    "            nn.Linear(in_features = 50, out_features = 3)\n",
    "        )\n",
    "\n",
    "        #6/8\n",
    "        self.NonLinearEncoder = nn.Sequential(\n",
    "            nn.Linear(in_features = 15, out_features = 128),\n",
    "            nn.Linear(in_features = 128, out_features = 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=6, out_features = 6)\n",
    "        )\n",
    "\n",
    "        #7/8\n",
    "        self.LinearDecoder = nn.Sequential(\n",
    "            nn.Linear(in_features = latent_features, out_features = 50),\n",
    "            nn.Linear(in_features = 50, out_features = 2),\n",
    "        )\n",
    "\n",
    "        #8/8\n",
    "        self.NonLinearDecoder = nn.Sequential(\n",
    "            nn.Linear(in_features = 3, out_features = 128),\n",
    "            nn.Linear(in_features = 128, out_features = 6),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 6, out_features = 10),\n",
    "        )\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "\n",
    "    #Funktion fra Thea, ved ikke helt hvor hun har den fra - ligner dog equation (2)\n",
    "   \n",
    "    \n",
    "    def prior(self, batch_size:int=1, )-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        #Expand prior_params til at være samme antal rækker som i den valgte batch size således at der fås\n",
    "        #en tensor med dimensionerne (batch_size, 2*latent_features), som så kan udfyldes med\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        #chunk opdeler prior_params i to dele, de første 0-latent_features kollonner indeholder mu og \n",
    "        #de sidste n_latent_features inde holder sigmaerne. Nu er der to tensors, som begge har dim\n",
    "        #(batch_size, n_latent_features). Værdierne i disse tensors, kan bruges til at sample hvordan\n",
    "        #latent space ser ud (Den der hedder 'latent interpolations' i plots i bunden.)\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        #BEMÆRK at at det er log_sigma, dvs. at når den inputtes i ReparameterizedDiagonalGaussian så fås mu = 0, sigma = 1\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    #NN 4/7\n",
    "    def encoder(self, x, y, e):\n",
    "\n",
    "        #x = relu(self.encoderCNN1(x))\n",
    "        #x = relu(self.encoderCNN2(x))\n",
    "        #x = relu(self.encoderCNN3(x))\n",
    "        #x = x.view(x.size(0), -1) #NN 4/7\n",
    "        \n",
    "        #ye = torch.cat((y, e), dim = 1)\n",
    "        # ye = ye.to(torch.float32)\n",
    "        # ye = self.YEencoder(ye) #NN 5/7\n",
    "\n",
    "        xye = torch.cat((x,y,e), dim = 1)\n",
    "        #print(xye.size())\n",
    "        xye = self.NonLinearEncoder(xye)\n",
    "        # xye = self.XYEmerger(xye) #NN 6/7\n",
    "        #mu, log_sigma =  x.chunk(2, dim=-1)\n",
    "        \n",
    "        return xye\n",
    "   \n",
    "    #NN 7/7\n",
    "    def decoder(self, z):\n",
    "        x = self.NonLinearDecoder(z)\n",
    "        return x\n",
    "        \n",
    "    def posterior(self, x:Tensor, y:Tensor, e:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x, y, e)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        #print(z.size())\n",
    "        px_loc = self.decoder(z)\n",
    "        #print(px_loc.size())\n",
    "        #px_loc = px_loc.view(-1, *self.input_shape) # reshape the output #old\n",
    "        #sandsynlighedsfordeling der giver 1 eller 0, baseret p� log-odds givet i logits input fra p(x|z).\n",
    "        #Dvs. at px_logits angiver sandsynligheden for at det givne pixel er henholdsvist r�d,gr�n,bl�. Pixel v�rdien\n",
    "        #er enten 0 eller 1. N�r man sampler fra bernoulli fordelingen f�s dermed et billede, som givet z, giver en figur,\n",
    "        #som er bestemt af de sandsynligheder der er i px_logits (p(x|z)). Dvs. at for et givet latents space, kan en\n",
    "        #figur/et tal reproduceres ud fra de beregnede sandsynligheder og den efterf�lgende sample fra Bernoulli fordelingen.\n",
    "        #return Bernoulli(logits=px_loc, validate_args=False)\n",
    "        return Normal(loc=px_loc, scale = 0.1*torch.ones(px_loc.shape))\n",
    "        \n",
    "\n",
    "    def forward(self, x, y, e) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        #### Run through ENCODER and calculate mu and sigma for latent space sampling\n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x, y, e)\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        #### LATENT SPACE\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        #### DECODER\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "\n",
    "        return {'px': px, 'pz': pz,'qz': qz, 'z': z}\n",
    "\n",
    "    def VariationalInference(self, x, y, e):\n",
    "        \n",
    "        # forward pass through the model to get the encoder and decoder outputs\n",
    "        parameters_and_latent_space = self.forward(x, y, e)\n",
    "\n",
    "        # unpack encoder parameters from (px), decoder parameters (qz) and the latent space (z)\n",
    "        px, pz, qz, z = [parameters_and_latent_space[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        #px_ze, qz_xye, z = [parameters_and_latent_space[k] for k in [\"px\", \"qz\", \"z\"]]\n",
    "\n",
    "        \n",
    "\n",
    "        # DEFINE THE PRIOR p(z)\n",
    "        #### PRIOR\n",
    "        z_temp = z.detach().requires_grad_(requires_grad = True)\n",
    "        \n",
    "        #pz_params = self.prior_params(z_temp, y.detach(), e.detach())\n",
    "        #prior calculation from page 28 (til differentiering ifm. eq. 64 skal dette ikke v�re logget, men n�r\n",
    "        # elbo loss beregnes, skal man huske at tage log at denne v�rdi jvf. p. 28)\n",
    "        #pz_ye =  self.prior_pdf(z_temp, *pz_params)\n",
    "        #OBS: HER SKAL DER IKKE ANVENDES LOG-PROBS AF PZ_YE JVF. SM DELEN AF EQ. (64) P� SIDE 28\n",
    "        #dpz_ye = torch.autograd.grad(pz_ye.mean(), z_temp, create_graph = True, retain_graph=True)[0]\n",
    "\n",
    "        #ddpz_ye = torch.autograd.grad(dpz_ye.mean(), z_temp, create_graph = True, retain_graph=True)[0] #changed sum() to mean()\n",
    "        #ddpz_ye = torch.autograd.grad(dpz_ye.sum(), z_temp, create_graph = True, retain_graph=True)[0] #original\n",
    "\n",
    "        #### SM loss SM loss SM loss SM loss SM loss SM loss SM loss SM loss ####\n",
    "        #Calculation from page 28 equation 64\n",
    "        #SM = (ddpz_ye + 0.5 * dpz_ye.pow(2)).sum(1)\n",
    "        #### SM loss SM loss SM loss SM loss SM loss SM loss SM loss SM loss ####\n",
    "\n",
    "        #### ELBO loss ELBO loss ELBO loss ELBO loss ELBO loss ELBO loss ELBO loss ####\n",
    "        # evaluate log probabilities\n",
    "        # Skal jvf. s. 32 �ndres til at v�re en normal fordeling i stedet for en\n",
    "        # bernoulli fordeling, og her skal mean v�re outputtet af decoderen og varians skal\n",
    "        #blot s�ttes til at v�re 0.01\n",
    "        #log_px_ze = reduce(px.log_prob(x)) #log(p(x|z)): Sandsynligheden for at observere vores input variabel x\n",
    "        #givet vores latent space (tjekker modellens evne til at rekonstruere sig selv, ved at maximere sandsynlig-\n",
    "        #heden for at observere inputtet selv, givet det konstruerede latent space.\n",
    "        \n",
    "        ####(old)log_pz = reduce(pz.log_prob(z)) #log(p(z)): \n",
    "        #log_pz_ye = torch.log(pz_ye)#Sandsynligheden for at observere vores latent space, givet at\n",
    "        #latent space f�lger en standard-normal fordeling (Jo h�jere sandsynlighed jo bedre)\n",
    "        \n",
    "        #log_qz_xye = reduce(pz.log_prob(z)) #log(q(z|x)): Sandsynligheden for at generere netop dette latent space givet \n",
    "        #vores input billede x. Denne v�rdi skal helst v�re lav?\n",
    "        \n",
    "        # compute the ELBO: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        #########################################################################################################\n",
    "        # Reconstruction loss: E_q [ log p(x|z) ]\n",
    "        # Regularization term: \\beta * D_KL(q(z|x) | p(z))` => Fors�ger at tvinge fordelingen q(z|x) mod N(0,1)?\n",
    "        #########################################################################################################\n",
    "        \n",
    "        \"\"\"\n",
    "            Pz,qz, px for the VAE\n",
    "        \"\"\"\n",
    "        log_px = reduce(px.log_prob(x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "\n",
    "        kl = log_qz- log_pz\n",
    "\n",
    "        #kl = log_qz_xye - qz.detach() #OBS: DENNE BEREGNING SKAL M�SKE �NDRES TIL AT TAGE LOG AF PZ_YE JVF P. 28.\n",
    "        \n",
    "        elbo = log_px - kl\n",
    "        ####\n",
    "        beta_elbo = log_px - self.beta * kl\n",
    "        # loss\n",
    "        loss = -(beta_elbo.mean())\n",
    "        #print(\"SM: {}. elbo: {} = log_px_ze: {} + log_pz_ye: {} - log_qz_xye: {}\".format(SM.mean(), elbo.mean(),log_px_ze.mean(),pz_ye.mean(),log_qz_xye.mean()))\n",
    "\n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "        \n",
    "        outputs = parameters_and_latent_space\n",
    "        return loss, diagnostics, outputs\n",
    "\n",
    "latent_features = 3 #Husk at opdater denne parameter nede i 'initialization', hvis den skal bruges i VAE loopet ogs�\n",
    "batch_size = 128\n",
    "vae = VAE(images[0].shape, latent_features)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss   | mean =    362.179, shape: []\n",
      "elbo   | mean =   -362.179, shape: [128]\n",
      "log_px | mean =   -361.555, shape: [128]\n",
      "kl     | mean =      0.624, shape: [128]\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_loader))\n",
    "images = sample[0]\n",
    "label = sample[1]#.reshape(-1,1)\n",
    "environment = sample[2]#.reshape(-1,1)\n",
    "\n",
    "x = images\n",
    "y = label\n",
    "e = environment\n",
    "\n",
    "loss, diagnostics, outputs = vae.VariationalInference(x.float(), y.float(), e.float())\n",
    "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
    "for key, tensor in diagnostics.items():\n",
    "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:00<00:00, 243.03it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 260.03it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 278.99it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 244.40it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 269.43it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 267.32it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 271.22it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 276.85it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 271.21it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 283.51it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 265.48it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 283.14it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 291.81it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 277.03it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 281.56it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 249.91it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 286.46it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 279.16it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 279.99it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 284.00it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 270.92it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 296.69it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 282.19it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 275.88it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 272.25it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 282.34it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 295.86it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 273.04it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 289.09it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 277.66it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 264.61it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 278.11it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 289.50it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 272.54it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 275.78it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 288.62it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 282.43it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 287.66it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 277.46it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 282.47it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 291.20it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 283.54it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 287.21it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 268.91it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 291.01it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 279.17it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 270.28it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 276.09it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 264.49it/s]\n",
      "100%|██████████| 118/118 [00:00<00:00, 261.77it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# VAE\n",
    "latent_features = 3 #Hyper parameter\n",
    "vae = VAE(images[0].shape, latent_features)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3) #Hyper parameter, tilf�j evt. weight_decay (L2 regularization)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "epoch = 0\n",
    "\n",
    "torch.manual_seed(2)\n",
    "num_epochs = 50 #hyper parametre\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)\n",
    "sample_counter = 0\n",
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for sample in tqdm(train_loader):\n",
    "        sample_counter += 1\n",
    "        \n",
    "        x = sample[0]\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample[1]#.reshape(-1,1)\n",
    "        y = y.to(device)\n",
    "\n",
    "        e = sample[2]#.reshape(-1,1)\n",
    "        e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        optimizer.zero_grad()\n",
    "        loss, diagnostics, outputs = vae.VariationalInference(x.float(), y.float(), e.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    px = outputs['px']\n",
    "    samples = px.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - PC algorithm based on latent variable\n",
    "\n",
    "(page 6-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU automatically detected. Setting SETTINGS.GPU to 0, and SETTINGS.NJOBS to cpu_count.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\site-packages\\cdt\\utils\\R.py:206\u001b[0m, in \u001b[0;36mlaunch_R_script\u001b[1;34m(template, arguments, output_function, verbose, debug)\u001b[0m\n\u001b[0;32m    204\u001b[0m         process \u001b[39m=\u001b[39m subprocess\u001b[39m.\u001b[39mPopen([\u001b[39mstr\u001b[39m(rpath), \u001b[39m\"\u001b[39m\u001b[39m--no-restore --no-save --no-site-file\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mstr\u001b[39m(scriptpath)],\n\u001b[0;32m    205\u001b[0m                                    stdout\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE, stderr\u001b[39m=\u001b[39msubprocess\u001b[39m.\u001b[39mPIPE)\n\u001b[1;32m--> 206\u001b[0m     process\u001b[39m.\u001b[39;49mwait()\n\u001b[0;32m    207\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\subprocess.py:1209\u001b[0m, in \u001b[0;36mPopen.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1208\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1209\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wait(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1210\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1211\u001b[0m     \u001b[39m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m     \u001b[39m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m     \u001b[39m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[0;32m   1214\u001b[0m     \u001b[39m# generated SIGINT and will exit rapidly.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\subprocess.py:1490\u001b[0m, in \u001b[0;36mPopen._wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1488\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1489\u001b[0m     \u001b[39m# API note: Returns immediately if timeout_millis == 0.\u001b[39;00m\n\u001b[1;32m-> 1490\u001b[0m     result \u001b[39m=\u001b[39m _winapi\u001b[39m.\u001b[39;49mWaitForSingleObject(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle,\n\u001b[0;32m   1491\u001b[0m                                          timeout_millis)\n\u001b[0;32m   1492\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39m==\u001b[39m _winapi\u001b[39m.\u001b[39mWAIT_TIMEOUT:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\site-packages\\cdt\\causality\\graph\\PC.py:310\u001b[0m, in \u001b[0;36mPC._run_pc\u001b[1;34m(self, data, fixedEdges, fixedGaps, verbose)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marguments[\u001b[39m'\u001b[39m\u001b[39m{E_EDGES}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mFALSE\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m--> 310\u001b[0m     pc_result \u001b[39m=\u001b[39m launch_R_script(Path(\u001b[39m\"\u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m/R_templates/pc.R\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mdirname(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mrealpath(\u001b[39m__file__\u001b[39;49m)))),\n\u001b[0;32m    311\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marguments, output_function\u001b[39m=\u001b[39;49mretrieve_result, verbose\u001b[39m=\u001b[39;49mverbose)\n\u001b[0;32m    312\u001b[0m \u001b[39m# Cleanup\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\site-packages\\cdt\\utils\\R.py:210\u001b[0m, in \u001b[0;36mlaunch_R_script\u001b[1;34m(template, arguments, output_function, verbose, debug)\u001b[0m\n\u001b[0;32m    209\u001b[0m         rmtree(base_dir)\n\u001b[1;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [32], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m df\u001b[39m.\u001b[39mhead()\n\u001b[0;32m     52\u001b[0m pc \u001b[39m=\u001b[39m PC(alpha \u001b[39m=\u001b[39m \u001b[39m0.1\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m pc_output \u001b[39m=\u001b[39m pc\u001b[39m.\u001b[39;49mpredict(df)\n\u001b[0;32m     55\u001b[0m nx\u001b[39m.\u001b[39mdraw(pc_output, with_labels\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, font_weight\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbold\u001b[39m\u001b[39m'\u001b[39m, font_size \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m)\n\u001b[0;32m     56\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32mc:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\site-packages\\cdt\\causality\\graph\\model.py:63\u001b[0m, in \u001b[0;36mGraphModel.predict\u001b[1;34m(self, df_data, graph, **kwargs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[39m\"\"\"Orient a graph using the method defined by the arguments.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[39mDepending on the type of `graph`, this function process to execute\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m   name of the variables in df_data\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     62\u001b[0m \u001b[39mif\u001b[39;00m graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 63\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_graph_from_data(df_data, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     64\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(graph, nx\u001b[39m.\u001b[39mDiGraph):\n\u001b[0;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39morient_directed_graph(df_data, graph, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\site-packages\\cdt\\causality\\graph\\PC.py:278\u001b[0m, in \u001b[0;36mPC.create_graph_from_data\u001b[1;34m(self, data, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marguments[\u001b[39m'\u001b[39m\u001b[39m{NJOBS}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnjobs)\n\u001b[0;32m    276\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marguments[\u001b[39m'\u001b[39m\u001b[39m{VERBOSE}\u001b[39;00m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\u001b[39m.\u001b[39mupper()\n\u001b[1;32m--> 278\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_pc(data, verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose)\n\u001b[0;32m    280\u001b[0m \u001b[39mreturn\u001b[39;00m nx\u001b[39m.\u001b[39mrelabel_nodes(nx\u001b[39m.\u001b[39mDiGraph(results),\n\u001b[0;32m    281\u001b[0m                         {idx: i \u001b[39mfor\u001b[39;00m idx, i \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data\u001b[39m.\u001b[39mcolumns)})\n",
      "File \u001b[1;32mc:\\Users\\Peetz\\miniconda3\\envs\\02456\\lib\\site-packages\\cdt\\causality\\graph\\PC.py:318\u001b[0m, in \u001b[0;36mPC._run_pc\u001b[1;34m(self, data, fixedEdges, fixedGaps, verbose)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     rmtree(run_dir)\n\u001b[1;32m--> 318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m\n\u001b[0;32m    319\u001b[0m rmtree(run_dir)\n\u001b[0;32m    320\u001b[0m \u001b[39mreturn\u001b[39;00m pc_result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Dummy code for the testing of the PC algorithm to ensure the model data is prepared in the right format\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cdt\n",
    "cdt.SETTINGS.rpath = r'C:\\Program Files\\R\\R-4.2.2\\bin/Rscript' # this path should point to your own R implementation !\n",
    "from cdt.causality.graph import PC\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "#names = np.array([\"A\",\"B\", \"C\", \"D\", \"E\"])\n",
    "#data_df = pd.DataFrame(data, columns = names)\n",
    "#pc_test = PC(CItest = 'gaussian', alpha = 0.05, verbose=False).create_graph_from_data(data_df[names[permutation]])\n",
    "\n",
    "#nx.draw(pc_test, with_labels=True, font_weight='bold')\n",
    "#plt.show()\n",
    "\n",
    "#X, Y, envs\n",
    "#cmnist = ConcatDataset([X, Y, E])\n",
    "#loader = DataLoader(cmnist, batch_size = 8000, batch_sampler = None)\n",
    "#dset = EnvDataset(X[:4000],Y[:4000],envs[:4000])\n",
    "#loader = DataLoader(dset, batch_size=4000, drop_last=True)\n",
    "loader = DataLoader(dset1, batch_size=15000)\n",
    "sample = next(iter(loader))\n",
    "\n",
    "x = sample[0]\n",
    "y = sample[1]#.reshape(-1,1)\n",
    "e = sample[2]#.reshape(-1,1)\n",
    "\n",
    "output = vae.forward(x.float(), y.float(), e.float())\n",
    "\n",
    "z = output['z']\n",
    "\n",
    "z_full = z\n",
    "x_full = x\n",
    "y_full = y\n",
    "e_full = e\n",
    "\n",
    "enviro = np.zeros((len(e),1))\n",
    "for i in range(0,4):\n",
    "    inde = e[:,i] == 1\n",
    "    enviro[inde] = i+1\n",
    "\n",
    "z_df = pd.DataFrame(z.detach())\n",
    "y_df = pd.DataFrame(y.detach())\n",
    "e_df = pd.DataFrame(enviro)\n",
    "\n",
    "df = z_df; df['Y'] = y_df; df['E'] = e_df\n",
    "df.columns = ['Z1', 'Z2', 'Z3', 'Y', 'E']\n",
    "df.head()\n",
    "\n",
    "pc = PC(alpha = 0.1)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc = PC(alpha = 0.2)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc = PC(alpha = 0.35)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc = PC(alpha = 0.4)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pc = PC(alpha = 0.6)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([\"r\", \"g\", \"b\", \"y\"])\n",
    "\n",
    "plt.scatter(df[\"Z1\"], df[\"Z3\"], c=df[\"E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(df[\"Z1\"], df[\"Z2\"], c=df[\"E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(df[\"Z2\"], df[\"Z3\"], c=df[\"E\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('02456')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "933f3aa8d9c429c2eee70035fa174da68e619e4a97d7c59a5ea775702747c954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
