{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from typing import *\n",
    "from IPython.display import Image, display, clear_output\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import math \n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus, relu\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import Bernoulli\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from plotting import make_vae_plots\n",
    "\n",
    "CMNIST_list = torch.load(\"data/ColoredMNIST/train1.pt\")\n",
    "CMNIST = pd.DataFrame(CMNIST_list, columns = [\"image\", \"label\"])\n",
    "\n",
    "images = CMNIST.iloc[:,0]\n",
    "labels_df = CMNIST.iloc[:,1]\n",
    "labels = torch.tensor(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "transform_to_tensor = ToTensor()\n",
    "transform_to_tensor(images[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.distributions import Distribution\n",
    "import torch.distributions as dist\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SynthNN(nn.Module):\n",
    "    def __init__(self, hidden_size = 6):\n",
    "        super().__init__()\n",
    "        self.function = nn.Sequential(\n",
    "            nn.Linear(in_features=2, out_features=hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=hidden_size, out_features=10),\n",
    "        )\n",
    "    def forward(self, Z):\n",
    "        return self.function(Z)\n",
    "\n",
    "\n",
    "def simulate_synthetic_data(samples, function):\n",
    "    envs1 = np.array([0.2, 2, 3, 5])\n",
    "    E = np.random.choice([0,1,2,3], size = samples)\n",
    "\n",
    "    env = envs1[E]\n",
    "    Z1 = env + np.random.normal(0, 1, size = samples)\n",
    "    Z2 = 2*env + np.random.normal(0, np.sqrt(2), size = samples)\n",
    "    Y = Z1 + Z2 + np.random.normal(0, 1, size = samples)\n",
    "    Z = np.stack([Z1, Z2], axis = 1)\n",
    "\n",
    "    if function == 'identity':\n",
    "        X = Z\n",
    "    elif function == 'linear':\n",
    "        S = np.random.normal(size = (2,10))\n",
    "        X = Z@S\n",
    "    elif function == 'nonlinear':\n",
    "        synthnn = SynthNN()\n",
    "        X = synthnn(torch.tensor(Z).float()).detach().numpy()\n",
    "    \n",
    "    return X, Y, E, Z, env\n",
    "\n",
    "class EnvDataset(Dataset):\n",
    "    def __init__(self, X, Y, E):\n",
    "        super().__init__()\n",
    "        self.X = torch.tensor(X)\n",
    "        self.Y = torch.tensor(Y).unsqueeze(1)\n",
    "        E = torch.tensor(E) #From Peter\n",
    "        # self.E = torch.nn.functional.one_hot(torch.tensor(E))\n",
    "        self.E = torch.nn.functional.one_hot(E.long())\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index], self.E[index] \n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.X))\n",
    "        \n",
    "model = SynthNN()\n",
    "#x, y, e, z, env = simulate_synthetic_data(100, \"nonlinear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate data\n",
    "#torch.manual_seed(1)\n",
    "X, Y, envs, Z, E = simulate_synthetic_data(20000, 'nonlinear')\n",
    "dset1 = EnvDataset(X[:15000],Y[:15000],envs[:15000])\n",
    "train_loader = DataLoader(dset1, batch_size=128)\n",
    "\n",
    "\n",
    "dset2 = EnvDataset(X[5000:],Y[5000:],envs[5000:])\n",
    "test_loader = DataLoader(dset2, batch_size=128)\n",
    "\n",
    "#test = next(iter(train_loader))\n",
    "#print(test[2])\n",
    "\n",
    "\n",
    "# Load a batch of images into memory\n",
    "sample = next(iter(train_loader))\n",
    "images = sample[0]\n",
    "label = sample[1]\n",
    "environment = sample[2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting base distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorDF = pd.DataFrame(E)\n",
    "cDF = colorDF.value_counts().rename_axis('unique_values').reset_index(name='counts')\n",
    "cDF =cDF.sort_values(\"unique_values\").reset_index()\n",
    "colors = []\n",
    "\n",
    "for col in E:\n",
    "    if col == cDF[\"unique_values\"][0]:\n",
    "        colors.append(\"red\")\n",
    "    elif col == cDF[\"unique_values\"][1]:\n",
    "        colors.append(\"green\")\n",
    "    elif col == cDF[\"unique_values\"][2]:\n",
    "        colors.append(\"blue\")\n",
    "    elif col == cDF[\"unique_values\"][3]:\n",
    "        colors.append(\"orange\")\n",
    "\n",
    "# Plotting base data\n",
    "orgFrame = pd.DataFrame(Z)\n",
    "plt.axis('off')\n",
    "# plt.scatter(orgFrame[0],orgFrame[1], orgFrame[2], c = colors).set_alpha(1)\n",
    "plt.scatter(orgFrame[0],orgFrame[1], c = colors).set_alpha(1) # Plotting w/ only two parents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgFrame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copied from exercises and adjusted to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Bernoulli(logits=torch.zeros((1000,)))\n",
    "\n",
    "\n",
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "    #def sample_epsilon(self) -> Tensor:\n",
    "    #    \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "    #    return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    #def sample(self) -> Tensor:\n",
    "    #    \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "    #    with torch.no_grad():\n",
    "    #        return self.rsample()\n",
    "        \n",
    "    #def rsample(self) -> Tensor:\n",
    "    #    \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "    #    self.z = torch.distributions.Normal(self.mu, self.sigma)\n",
    "    #    return self.z.rsample() # <- your code\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        eps = torch.empty_like(self.mu).normal_()\n",
    "        return self.mu + self.sigma * eps\n",
    "    # <- your code    \n",
    "        #return self.mu + self.sigma * self.sample_epsilon() # <- your code    \n",
    "    \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        return - ((z - self.mu)**2)/(2*self.sigma**2) - torch.log(self.sigma) - math.log(math.sqrt(2 * math.pi)) # <- your code\n",
    "    \n",
    "    #def log_prob(self, z:Tensor) -> Tensor:\n",
    "    #    \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "    #    dummy = self.rsample()\n",
    "    #    return self.z.log_prob(z) # <- your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = np.prod(input_shape)\n",
    "        \n",
    "\n",
    "        # Inference Network\n",
    "        # Encode the observation `x` into the parameters of the posterior distribution\n",
    "        # `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(in_features=self.observation_features, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            # A Gaussian is fully characterised by its mean \\mu and variance \\sigma**2\n",
    "            nn.Linear(in_features=64, out_features=2*latent_features) # <- note the 2*latent_features\n",
    "        )\n",
    "        \n",
    "        # Generative Model\n",
    "        # Decode the latent sample `z` into the parameters of the observation model\n",
    "        # `p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(in_features=latent_features, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=64, out_features=128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=128, out_features=256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=256, out_features=self.observation_features)\n",
    "        )\n",
    "        \n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        \n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_logits = self.decoder(z)\n",
    "        px_logits = px_logits.view(-1, *self.input_shape) # reshape the output\n",
    "        return Bernoulli(logits=px_logits, validate_args=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        # flatten the input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "        \n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "        \n",
    "        # sample the prior \n",
    "        z = pz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'z': z}\n",
    "\n",
    "\n",
    "latent_features = 2\n",
    "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        #The input model in our case is the VAE and the x:tensor is our images.\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x)) #log(p(x|z)): Sandsynligheden for at observere vores input variabel x\n",
    "        #givet vores latent space (tjekker modellens evne til at rekonstruere sig selv, ved at maximere sandsynlig-\n",
    "        #heden for at observere inputtet selv, givet det konstruerede latent space.\n",
    "        log_pz = reduce(pz.log_prob(z)) #log(p(z)): Sandsynligheden for at observere vores latent space, givet at\n",
    "        #latent space følger en standard-normal fordeling (Jo højere sandsynlighed jo bedre)\n",
    "        log_qz = reduce(qz.log_prob(z)) #log(q(z|x)): Sandsynligheden for at generere netop dette latent space givet \n",
    "        #vores input billede x. Denne værdi skal helst være lav?\n",
    "        \n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        #########################################################################################################\n",
    "        # Reconstruction loss: E_q [ log p(x|z) ]\n",
    "        # Regularization term: \\beta * D_KL(q(z|x) | p(z))` => Forsøger at tvinge fordelingen q(z|x) mod N(0,1)?\n",
    "        #########################################################################################################\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl\n",
    "        beta_elbo = log_px - self.beta * kl\n",
    "        \n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vi = VariationalInference(beta=1)\n",
    "loss, diagnostics, outputs = vi(vae, images)\n",
    "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
    "for key, tensor in diagnostics.items():\n",
    "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# VAE\n",
    "latent_features = 2 #Hyper parameter\n",
    "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "beta = 1 #Hyper parameter\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3) #Hyper parameter, tilføj evt. weight_decay (L2 regularization)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Better implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 200 #hyper parametre\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)\n",
    "\n",
    "# training..\n",
    "for i in tqdm(range(num_epochs)):\n",
    "    epoch+= 1\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for sample in train_loader:\n",
    "        x = images\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        \n",
    "        # Just load a single batch from the test loader\n",
    "        sample = next(iter(test_loader))\n",
    "        x = images\n",
    "        y = label\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "        \n",
    "        \"\"\"\n",
    "            Debugging variables\n",
    "        \"\"\"\n",
    "        # print(training_data['elbo'][epoch-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - PC algorithm based on latent variable\n",
    "\n",
    "(page 6-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy code for the testing of the PC algorithm to ensure the model data is prepared in the right format\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cdt\n",
    "cdt.SETTINGS.rpath = r'C:\\Program Files\\R\\R-4.2.2\\bin/Rscript' # this path should point to your own R implementation !\n",
    "from cdt.causality.graph import PC\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "#names = np.array([\"A\",\"B\", \"C\", \"D\", \"E\"])\n",
    "#data_df = pd.DataFrame(data, columns = names)\n",
    "#pc_test = PC(CItest = 'gaussian', alpha = 0.05, verbose=False).create_graph_from_data(data_df[names[permutation]])\n",
    "\n",
    "#nx.draw(pc_test, with_labels=True, font_weight='bold')\n",
    "#plt.show()\n",
    "\n",
    "#X, Y, envs\n",
    "#cmnist = ConcatDataset([X, Y, E])\n",
    "#loader = DataLoader(cmnist, batch_size = 8000, batch_sampler = None)\n",
    "#dset = EnvDataset(X[:4000],Y[:4000],envs[:4000])\n",
    "#loader = DataLoader(dset, batch_size=4000, drop_last=True)\n",
    "loader = DataLoader(dset1, batch_size=15000)\n",
    "sample = next(iter(loader))\n",
    "\n",
    "x = sample[0]\n",
    "y = sample[1]#.reshape(-1,1)\n",
    "e = sample[2]#.reshape(-1,1)\n",
    "\n",
    "# output = vae.forward(x.float(), y.float(), e.float())\n",
    "output = vae.forward(x.float())\n",
    "\n",
    "z = output['z']\n",
    "\n",
    "z_full = z\n",
    "x_full = x\n",
    "y_full = y\n",
    "e_full = e\n",
    "\n",
    "enviro = np.zeros((len(e),1))\n",
    "for i in range(0,4):\n",
    "    inde = e[:,i] == 1\n",
    "    enviro[inde] = i+1\n",
    "\n",
    "z_df = pd.DataFrame(z.detach())\n",
    "y_df = pd.DataFrame(y.detach())\n",
    "e_df = pd.DataFrame(enviro)\n",
    "\n",
    "df = z_df; df['Y'] = y_df; \n",
    "#df_e['E'] = e_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.columns = ['Y','Z1', 'Z2']\n",
    "df.head()\n",
    "\n",
    "pc = PC(alpha = 0.005)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = []\n",
    "\n",
    "for col in df[\"E\"]:\n",
    "\n",
    "    if col == (1.0):\n",
    "\n",
    "        colors.append(\"red\")\n",
    "\n",
    "    elif col == (2.0):\n",
    "\n",
    "        colors.append(\"green\")\n",
    "\n",
    "    elif col == (3.0):\n",
    "\n",
    "        colors.append(\"blue\")\n",
    "\n",
    "    elif col == (4.0):\n",
    "\n",
    "        colors.append(\"orange\")\n",
    "\n",
    "\n",
    "\n",
    "df[\"colour\"] = colors\n",
    "\n",
    "\n",
    "\n",
    "colors = np.array([\"r\", \"g\", \"b\", \"y\"])\n",
    "plt.scatter(df[\"Z1\"], df[\"Z2\"], c=df[\"colour\"]).set_alpha(0.1)\n",
    "plt.axis(\"off\")\n",
    "plt.savefig('Z1Z3_iVAE.png', transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('02456')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "933f3aa8d9c429c2eee70035fa174da68e619e4a97d7c59a5ea775702747c954"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
