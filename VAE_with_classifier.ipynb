{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io, transform\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "from typing import *\n",
    "from IPython.display import Image, display, clear_output\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "import math \n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus, relu\n",
    "from torch.distributions import Distribution\n",
    "from torch.distributions import Bernoulli, Normal\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "from functools import reduce\n",
    "\n",
    "from plotting import make_vae_plots\n",
    "\n",
    "CMNIST_list = torch.load(\"data/ColoredMNIST/train1.pt\")\n",
    "CMNIST = pd.DataFrame(CMNIST_list, columns = [\"image\", \"label\"])\n",
    "\n",
    "images = CMNIST.iloc[:,0]\n",
    "labels_df = CMNIST.iloc[:,1]\n",
    "labels = torch.tensor(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "transform_to_tensor = ToTensor()\n",
    "transform_to_tensor(images[0]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_CMNIST(image, label, environment):\n",
    "    \"\"\"Show image with landmarks\"\"\"\n",
    "    plt.figure\n",
    "    plt.imshow(image)\n",
    "    plt.text(1,2,\"Label: {}\".format(label), backgroundcolor = \"white\",\n",
    "             color = \"black\", fontsize = 6)\n",
    "    plt.text(1,5,\"Environment: {}\".format(environment), backgroundcolor = \"white\",\n",
    "             color = \"black\", fontsize = 6)\n",
    "    plt.axis('off')\n",
    "    plt.show\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "show_CMNIST(images[0], labels[0], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMNISTDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, pt_file, environment=None, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pt_file (string): Path to the pt file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "            environment (integer): Integer indicating the environment the data comes from\n",
    "        \"\"\"\n",
    "        self.CMNIST_frame = pd.DataFrame(torch.load(pt_file))\n",
    "        self.transform = transform\n",
    "        self.environment = environment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.CMNIST_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = self.CMNIST_frame.iloc[idx, 0]\n",
    "        label = self.CMNIST_frame.iloc[idx, 1]\n",
    "        sample = {'image': image, 'label': label, \"environment\": self.environment}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = {'image': self.transform(image), 'label': label, \"environment\": self.environment}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnist_data = CMNISTDataset(pt_file = \"data/ColoredMNIST/train1.pt\", environment = 1)\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "for i in range(len(cmnist_data)):\n",
    "    sample = cmnist_data[i]\n",
    "\n",
    "    #print(i, sample['image'].shape, sample['label'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 4, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Sample #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    show_CMNIST(**sample)\n",
    "\n",
    "    if i == 3:\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmnist_data1 = CMNISTDataset(pt_file = \"data/ColoredMNIST/train1.pt\"\n",
    "                            , environment = 1\n",
    "                            , transform = transform_to_tensor)\n",
    "cmnist_data2 = CMNISTDataset(pt_file = \"data/ColoredMNIST/train2.pt\"\n",
    "                            , environment = 2\n",
    "                            , transform = transform_to_tensor)\n",
    "train_loader = DataLoader(ConcatDataset([cmnist_data1, cmnist_data2]), batch_size=256, num_workers=0)\n",
    "\n",
    "cmnist_test_data = CMNISTDataset(pt_file = \"data/ColoredMNIST/test.pt\"\n",
    "                                 , environment = 3\n",
    "                                 , transform = transform_to_tensor)\n",
    "test_loader = DataLoader(cmnist_test_data, batch_size = 512, num_workers = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(sample_batched):\n",
    "    \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n",
    "    images_batch, landmarks_batch = \\\n",
    "            sample_batched['image'], sample_batched['label']\n",
    "    batch_size = len(images_batch)\n",
    "    im_size = images_batch.size(2)\n",
    "\n",
    "    grid = utils.make_grid(images_batch)\n",
    "    plt.imshow(grid.numpy().transpose((1, 2, 0)))\n",
    "\n",
    "for i_batch, sample_batched in enumerate(train_loader):\n",
    "    print(i_batch, sample_batched['image'].size(),\n",
    "          sample_batched['label'].size())\n",
    "\n",
    "    # observe 4th batch and stop.\n",
    "    if i_batch == 3:\n",
    "        plt.figure()\n",
    "        show_batch(sample_batched)\n",
    "        plt.axis('off')\n",
    "        plt.ioff()\n",
    "        plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Copied from exercises and adjusted to our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Bernoulli(logits=torch.zeros((1000,)))\n",
    "\n",
    "\n",
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "    #def sample_epsilon(self) -> Tensor:\n",
    "    #    \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "    #    return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    #def sample(self) -> Tensor:\n",
    "    #    \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "    #    with torch.no_grad():\n",
    "    #        return self.rsample()\n",
    "        \n",
    "    #def rsample(self) -> Tensor:\n",
    "    #    \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "    #    self.z = torch.distributions.Normal(self.mu, self.sigma)\n",
    "    #    return self.z.rsample() # <- your code\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        eps = torch.empty_like(self.mu).normal_()\n",
    "        return self.mu + self.sigma * eps\n",
    "    # <- your code    \n",
    "        #return self.mu + self.sigma * self.sample_epsilon() # <- your code    \n",
    "    \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        return - ((z - self.mu)**2)/(2*self.sigma**2) - torch.log(self.sigma) - math.log(math.sqrt(2 * math.pi)) # <- your code\n",
    "    \n",
    "    #def log_prob(self, z:Tensor) -> Tensor:\n",
    "    #    \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "    #    dummy = self.rsample()\n",
    "    #    return self.z.log_prob(z) # <- your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a few CMNIST examples\n",
    "f, axarr = plt.subplots(4, 16, figsize=(16, 4))\n",
    "\n",
    "# Load a batch of images into memory\n",
    "sample = next(iter(train_loader))\n",
    "images = sample['image']\n",
    "label = sample['label']\n",
    "environment = sample['environment']\n",
    "\n",
    "for i, ax in enumerate(axarr.flat):\n",
    "    ax.imshow(images[i].permute(1,2,0))\n",
    "    ax.text(1,2,\"Label: {}\".format(label[i]), backgroundcolor = \"white\",\n",
    "             color = \"black\", fontsize = 6)\n",
    "    ax.text(1,28,\"Environment: {}\".format(environment[i]), backgroundcolor = \"white\",\n",
    "             color = \"black\", fontsize = 6)\n",
    "    ax.axis('off')\n",
    "    \n",
    "plt.suptitle('MNIST handwritten digits')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int) -> None:\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.observation_features = np.prod(input_shape)\n",
    "        \n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        ## setting the prior to a vector consisting of zeros with dimensions (1,2*latent_features)\n",
    "        self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        \n",
    "         #Inference Network\n",
    "         #Encode the observation `x` into the parameters of the posterior distribution\n",
    "         #`q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoderCNN1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.encoderCNN2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.encoderCNN3 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.encoderFFN2 = nn.Linear(in_features = 32 * 4 * 4, out_features=32 * 4 * 4)\n",
    "        self.encoderFFN = nn.Linear(in_features = 32 * 4 * 4, out_features=2*latent_features)\n",
    "        \n",
    "        #Original network (baseline comparison)\n",
    "        #self.encoderFFN2 = nn.Linear(in_features = self.observation_features, out_features=256)\n",
    "        #self.encoderFFN3 = nn.Linear(in_features = 256, out_features=128)\n",
    "        #self.encoderFFN = nn.Linear(in_features = 128, out_features=2*latent_features)\n",
    "        \n",
    "    \n",
    "         #Generative Model\n",
    "         #Decode the latent sample `z` into the parameters of the observation model\n",
    "         #`p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        #Original network (baseline comparison)\n",
    "        #self.decoderFFN = nn.Linear(in_features=latent_features, out_features = 128)\n",
    "        #self.decoderFFN3 = nn.Linear(in_features=128, out_features = 256)\n",
    "        #self.decoderFFN2 = nn.Linear(in_features = 256, out_features = self.observation_features)\n",
    "        \n",
    "        self.decoderFFN = nn.Linear(in_features=latent_features, out_features = 32 * 4 * 4)\n",
    "        self.decoderFFN2 = nn.Linear(in_features = 32 * 4 * 4, out_features = 32 * 4 * 4)\n",
    "        \n",
    "        self.decoderCNN1 = nn.ConvTranspose2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 0)\n",
    "        self.decoderCNN2 = nn.ConvTranspose2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 1)\n",
    "        self.decoderCNN3 = nn.ConvTranspose2d(in_channels = 32, out_channels = 3, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 1)\n",
    "        \n",
    "    #Define encoder and decoder functions which can be modified to include both a CNN and an ordinary FFN    \n",
    "    def encoder(self, x):\n",
    "        \n",
    "        #Add CNN encoder and flatten x\n",
    "        \n",
    "        x = relu(self.encoderCNN1(x))\n",
    "        x = relu(self.encoderCNN2(x))\n",
    "        x = relu(self.encoderCNN3(x))\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.relu(self.encoderFFN2(x))\n",
    "        #x = self.relu(self.encoderFFN3(x)) #Only for running \"Original\"\n",
    "        x = self.encoderFFN(x)\n",
    "        return x\n",
    "   \n",
    "    def decoder(self, z):\n",
    "        \n",
    "        x = self.relu(self.decoderFFN(z))\n",
    "        #x = self.relu(self.decoderFFN3(x)) #Only for running \"Original\"\n",
    "        x = self.relu(self.decoderFFN2(x))\n",
    "        \n",
    "        # reshape x and add CNN decoder\n",
    "        x = x.view(-1, 32, 4, 4)\n",
    "        \n",
    "        x = relu(self.decoderCNN1(x))\n",
    "        x = relu(self.decoderCNN2(x))\n",
    "        x = self.decoderCNN3(x)\n",
    "        return x\n",
    "        \n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        #Expand prior_params til at være samme antal rækker som i den valgte batch size således at der fås\n",
    "        #en tensor med dimensionerne (batch_size, 2*latent_features), som så kan udfyldes med\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        #chunk opdeler prior_params i to dele, de første 0-latent_features kollonner indeholder mu og \n",
    "        #de sidste n_latent_features inde holder sigmaerne. Nu er der to tensors, som begge har dim\n",
    "        #(batch_size, n_latent_features). Værdierne i disse tensors, kan bruges til at sample hvordan\n",
    "        #latent space ser ud (Den der hedder 'latent interpolations' i plots i bunden.)\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        #BEMÆRK at at det er log_sigma, dvs. at når den inputtes i ReparameterizedDiagonalGaussian så fås mu = 0, sigma = 1\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_logits = self.decoder(z)\n",
    "        px_logits = px_logits.view(-1, *self.input_shape) # reshape the output #old\n",
    "        #sandsynlighedsfordeling der giver 1 eller 0, baseret på log-odds givet i logits input fra p(x|z).\n",
    "        #Dvs. at px_logits angiver sandsynligheden for at det givne pixel er henholdsvist rød,grøn,blå. Pixel værdien\n",
    "        #er enten 0 eller 1. Når man sampler fra bernoulli fordelingen fås dermed et billede, som givet z, giver en figur,\n",
    "        #som er bestemt af de sandsynligheder der er i px_logits (p(x|z)). Dvs. at for et givet latents space, kan en\n",
    "        #figur/et tal reproduceres ud fra de beregnede sandsynligheder og den efterfølgende sample fra Bernoulli fordelingen.\n",
    "        return Bernoulli(logits=px_logits, validate_args=False)\n",
    "        \n",
    "\n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        ###############################################################################################\n",
    "        # flatten the input\n",
    "        #x = x.view(x.size(0), -1) #outcommented as part of adding CNN\n",
    "        \n",
    "        #### Run through ENCODER and calculate mu and sigma for latent space sampling\n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        #### LATENT SPACE\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        #### DECODER\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        ###############################################################################################\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        #(Indgår i beregning af kl-term (regularisering) ifm. ELBO) - og bruges også til interpolations visualisering\n",
    "        #til sidst.\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "    def sample_from_prior(self, batch_size:int=100):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "        \n",
    "        # Laver bare reconstruction baseret på latent space\n",
    "        #Kan evt. fjernes. Anvendes bare til at vise hvor god modellen er til at generere data baseret på\n",
    "        #latent space genererede data. Funktionen anvendes kun i make_vae_plots.\n",
    "        \n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "        \n",
    "        # sample the prior \n",
    "        z = pz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'z': z}\n",
    "\n",
    "\n",
    "latent_features = 10 #Husk at opdater denne parameter nede i 'initialization', hvis den skal bruges i VAE loopet også\n",
    "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
    "print(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ELBO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        #The input model in our case is the VAE and the x:tensor is our images.\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x)) #log(p(x|z)): Sandsynligheden for at observere vores input variabel x\n",
    "        #givet vores latent space (tjekker modellens evne til at rekonstruere sig selv, ved at maximere sandsynlig-\n",
    "        #heden for at observere inputtet selv, givet det konstruerede latent space.\n",
    "        log_pz = reduce(pz.log_prob(z)) #log(p(z)): Sandsynligheden for at observere vores latent space, givet at\n",
    "        #latent space følger en standard-normal fordeling (Jo højere sandsynlighed jo bedre)\n",
    "        log_qz = reduce(qz.log_prob(z)) #log(q(z|x)): Sandsynligheden for at generere netop dette latent space givet \n",
    "        #vores input billede x. Denne værdi skal helst være lav?\n",
    "        \n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        #########################################################################################################\n",
    "        # Reconstruction loss: E_q [ log p(x|z) ]\n",
    "        # Regularization term: \\beta * D_KL(q(z|x) | p(z))` => Forsøger at tvinge fordelingen q(z|x) mod N(0,1)?\n",
    "        #########################################################################################################\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl\n",
    "        beta_elbo = log_px - self.beta * kl\n",
    "        \n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vi = VariationalInference(beta=1)\n",
    "loss, diagnostics, outputs = vi(vae, images)\n",
    "print(f\"{'loss':6} | mean = {loss:10.3f}, shape: {list(loss.shape)}\")\n",
    "for key, tensor in diagnostics.items():\n",
    "    print(f\"{key:6} | mean = {tensor.mean():10.3f}, shape: {list(tensor.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# VAE\n",
    "latent_features = 10 #Hyper parameter\n",
    "vae = VariationalAutoencoder(images[0].shape, latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "beta = 1 #Hyper parameter\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=1e-3) #Hyper parameter, tilføj evt. weight_decay (L2 regularization)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 100 #hyper parametre\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "vae = vae.to(device)\n",
    "\n",
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    vae.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for sample in train_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # Evaluate on a single batch, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        vae.eval()\n",
    "        \n",
    "        # Just load a single batch from the test loader\n",
    "        sample = next(iter(test_loader))\n",
    "        x = sample['image']\n",
    "        y = sample['label']\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(vae, x)\n",
    "        \n",
    "        # gather data for the validation step\n",
    "        for k, v in diagnostics.items():\n",
    "            validation_data[k] += [v.mean().item()]\n",
    "    \n",
    "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
    "    make_vae_plots(vae, x, y, outputs, training_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(vae.state_dict(), '100epochs_VAE_flipped.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae.load_state_dict(torch.load('100epochs_VAE_flipped.pt', map_location=device))\n",
    "#vae = vae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qz = vae.posterior(sample['image'])\n",
    "z = qz.rsample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - PC algorithm based on latent variable\n",
    "\n",
    "(page 6-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy code for the testing of the PC algorithm to ensure the model data is prepared in the right format\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cdt\n",
    "cdt.SETTINGS.rpath = 'C:/Program Files/R/R-4.2.2/bin/Rscript' # this path should point to your own R implementation !\n",
    "from cdt.causality.graph import PC\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "#names = np.array([\"A\",\"B\", \"C\", \"D\", \"E\"])\n",
    "#data_df = pd.DataFrame(data, columns = names)\n",
    "#pc_test = PC(CItest = 'gaussian', alpha = 0.05, verbose=False).create_graph_from_data(data_df[names[permutation]])\n",
    "\n",
    "#nx.draw(pc_test, with_labels=True, font_weight='bold')\n",
    "#plt.show()\n",
    "\n",
    "### Implementing PC algorithm on CMNIST iVAE result ###\n",
    "#load full CMNIST dataset --- determine causal latent variables based on training data\n",
    "# -> assume ALL test data are reserved to after all training is done.\n",
    "#cmnist_data1 = CMNISTDataset(pt_file = \"data/ColoredMNIST/train1.pt\"\n",
    "#                            , environment = 1\n",
    "#                            , transform = transform_to_tensor)\n",
    "#cmnist_data2 = CMNISTDataset(pt_file = \"data/ColoredMNIST/train2.pt\"\n",
    "#                            , environment = 2\n",
    "#                           , transform = transform_to_tensor)\n",
    "\n",
    "#cmnist = ConcatDataset([cmnist_data1, cmnist_data2])\n",
    "#loader = DataLoader(cmnist, batch_size = 40000, batch_sampler = None)\n",
    "\n",
    "sample = next(iter(loader))\n",
    "\n",
    "x = sample['image']\n",
    "y = sample['label'].reshape(-1,1)\n",
    "e = sample['environment'].reshape(-1,1)\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "e = e.to(device)\n",
    "\n",
    "vae = vae.to(device)\n",
    "\n",
    "output = vae.forward(x)\n",
    "\n",
    "z = output['z']\n",
    "\n",
    "z_full = z\n",
    "x_full = x\n",
    "y_full = y\n",
    "e_full = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qz = output['qz']\n",
    "z_sample1 = qz.rsample()\n",
    "z_sample2 = qz.rsample()\n",
    "z_sample3 = qz.rsample()\n",
    "z_sample4 = qz.rsample()\n",
    "z_sample5 = qz.rsample()\n",
    "z_sample6 = qz.rsample()\n",
    "z_sample7 = qz.rsample()\n",
    "z_sample8 = qz.rsample()\n",
    "z_sample9 = qz.rsample()\n",
    "z_sample10 = qz.rsample()\n",
    "\n",
    "y = y.detach()\n",
    "e = e.detach()\n",
    "\n",
    "\n",
    "\n",
    "y_df = pd.DataFrame(torch.cat((y,y,y,y,y,y,y,y,y,y), dim = 0))\n",
    "z_df = pd.DataFrame(torch.cat((z_sample1.detach(),z_sample2.detach(),z_sample3.detach(),z_sample4.detach(),z_sample5.detach(),\n",
    "    z_sample6.detach(),z_sample7.detach(),z_sample8.detach(),z_sample9.detach(),z_sample10.detach(),), dim = 0))\n",
    "df = y_df#; df = z_df; #df = z_df;  \n",
    "df['Z1'] = z_df[0]\n",
    "df['Z2'] = z_df[1]\n",
    "df['Z3'] = z_df[2]\n",
    "df['Z4'] = z_df[3]\n",
    "df['Z5'] = z_df[4]\n",
    "df['Z6'] = z_df[5]\n",
    "df['Z7'] = z_df[6]\n",
    "df['Z8'] = z_df[7]\n",
    "df['Z9'] = z_df[8]\n",
    "df['Z10'] = z_df[9]\n",
    "\n",
    "\n",
    "df.columns = ['Y','Z1', 'Z2', 'Z3', 'Z4', 'Z5', 'Z6', 'Z7', 'Z8', 'Z9', 'Z10']\n",
    "df.head()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glasso = cdt.independence.graph.Glasso()\n",
    "\n",
    "skeleton = glasso.predict(df)\n",
    "\n",
    "model_pc = cdt.causality.graph.PC(alpha =0.05)\n",
    "\n",
    "from causallearn.search.ConstraintBased.PC  import pc\n",
    "\n",
    "from causallearn.utils.cit import fisherz\n",
    "cg = pc(np.array(df), 0.1, kernelZ='Polynomial', approx=False, est_width='median')\n",
    "\n",
    "cg.draw_pydot_graph()\n",
    "#graph_pc = model_pc.predict(df, skeleton)\n",
    "\n",
    "#nx.draw(cg, with_labels=True, font_weight='bold', font_size = 8)\n",
    "#plt.savefig('PC_iVAE_alpha_005.png', transparent=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PC(alpha=0.05)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 16, node_size = 9000)\n",
    "plt.savefig('PC_iVAE_alpha_005.png', transparent=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parents(pc_array,y_pos,z_pos):\n",
    "    ind = np.where(pc_array[z_pos,y_pos] == 1)\n",
    "    ind_DAGpar = []\n",
    "    ind_par = []\n",
    "    for i in ind[0]:\n",
    "        if pc_array[y_pos,i] == 0:\n",
    "            ind_DAGpar.append(i)\n",
    "        else:\n",
    "            ind_par.append(i)\n",
    "    return ind_DAGpar, ind_par\n",
    "\n",
    "pc = PC(alpha=0.01)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 16, node_size = 9000)\n",
    "plt.savefig('PC_iVAE_alpha_005.png', transparent=True)\n",
    "plt.show()\n",
    "\n",
    "pc_array = pc._run_pc(df)\n",
    "parents = torch.tensor(find_parents(pc_array, 0, range(0,10))[0]).to(device)\n",
    "print(parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = torch.tensor(find_parents(pc_array, 0, range(0,10))[0]).to(device)\n",
    "parents = parents - 1 #For at følge rækkefølgen fra z\n",
    "print(parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found indices:\n",
    "\n",
    "#tensor([0, 3, 4, 6]) kører, og har alpha = 0.05 - Skal testes, men umiddelbart dårligt\n",
    "#tensor([3, 4, 6]) \n",
    "#parents = parents.to(\"cpu\")\n",
    "parents = torch.tensor([0,4,9])\n",
    "options = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "children = np.delete(options, parents)\n",
    "nParents = len(parents)\n",
    "nChildren = 10 - nParents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy function from week 4:\n",
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "#Building a classifier:\n",
    "\n",
    "class ClassifierVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        #Layers\n",
    "        self.input_layer = nn.Linear(in_features=3, out_features = 50)\n",
    "        self.layer1 = nn.Linear(in_features = 50, out_features = 100)\n",
    "        self.output_layer = nn.Linear(in_features = 100, out_features = 1)\n",
    "\n",
    "        #activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    #forward function(self, x):\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.sigmoid(self.output_layer(x))\n",
    "\n",
    "        return x\n",
    "    # \n",
    "\n",
    "classifierVAE = ClassifierVAE()\n",
    "classifierVAE.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "\n",
    "#Define loss function\n",
    "loss_fn = nn.BCELoss() \n",
    "#The adam algorithm automatically use momentum\n",
    "optimizer = optim.Adam(classifierVAE.parameters(), lr = 1e-4) \n",
    "\n",
    "#Test model\n",
    "#out = classifierVAE(z[1])\n",
    "#print(\"Output shape:\", out.size())\n",
    "#print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TRAINING THE CLASSIFIER\n",
    "epoch = 0\n",
    "num_epochs = 10 #hyper parametre\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\">> Using device: {device}\")\n",
    "\n",
    "vae = vae.to(device)\n",
    "classifierVAE = classifierVAE.to(device)\n",
    "parents = parents.to(device)\n",
    "\n",
    "# training..\n",
    "training_epoch_data = []\n",
    "test_epoch_data = []\n",
    "\n",
    "\n",
    "# training..\n",
    "training_epoch_data = []\n",
    "test_epoch_data = []\n",
    "test_acc, train_acc = [], []\n",
    "\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    training_batch_data = []\n",
    "    test_batch_data = []\n",
    "    classifierVAE.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for sample in train_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label'].to(torch.float32)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = vae.posterior(x)\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        outputVAE = classifierVAE(z).view(-1)\n",
    "\n",
    "        loss = loss_fn(outputVAE, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current batch\n",
    "        training_batch_data.append(loss)\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    training_epoch_data.append(torch.tensor(training_batch_data).mean())\n",
    "\n",
    "    print(\"Epoch: {}, Mean epoch loss: {}\".format(epoch, torch.tensor(training_batch_data).mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NF_ivae = NF_ivae.to(device)\n",
    "#indices = indices.to(device)\n",
    "#parents = parents.to(device)\n",
    "\n",
    "#Set iVAE to eval - the no.grad thing is not an issue as we are only using the encoder.\n",
    "#NF_ivae.eval()\n",
    "\n",
    "#TRAINING THE CLASSIFIER\n",
    "epoch = 0\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\">> Using device: {device}\")\n",
    "\n",
    "# training..\n",
    "training_epoch_data = []\n",
    "test_epoch_data = []\n",
    "test_acc, train_acc = [], []\n",
    "\n",
    "\n",
    "#Evaluate training\n",
    "with torch.no_grad():\n",
    "    classifierVAE.eval()\n",
    "    train_targs, train_preds = [], []\n",
    "    for sample in train_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label']\n",
    "        y = y.to(device)\n",
    "\n",
    "        #e = sample['environment']\n",
    "        #e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = vae.posterior(x)\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        output = classifierVAE(z).view(-1)\n",
    "        prediction = torch.round(output)\n",
    "\n",
    "        train_targs += list(y.cpu().numpy())\n",
    "        train_preds += list(prediction.data.cpu().numpy())\n",
    "\n",
    "    #Evaluating validation\n",
    "    val_targs, val_preds = [], []\n",
    "    for sample in test_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label']\n",
    "        y = y.to(device)\n",
    "\n",
    "        #e = sample['environment']\n",
    "        #e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = vae.posterior(x)\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        output = classifierVAE(z).view(-1)\n",
    "        prediction = torch.round(output)\n",
    "\n",
    "\n",
    "        val_targs += list(y.cpu().numpy())\n",
    "        val_preds += list(prediction.data.cpu().numpy())\n",
    "\n",
    "train_acc_cur = accuracy(Tensor(train_targs),Tensor(train_preds))\n",
    "train_acc.append(train_acc_cur)\n",
    "\n",
    "test_acc_cur = accuracy(Tensor(val_targs),Tensor(val_preds))\n",
    "test_acc.append(test_acc_cur)\n",
    "print(\"Epoch %2i : , Train acc %f, Valid acc %f\" % (\n",
    "            epoch, train_acc_cur, test_acc_cur))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(classifierVAE.state_dict(), 'classifierVAEv1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iVariationalAutoencoder(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int, device) -> None:\n",
    "        super(iVariationalAutoencoder, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        ## setting the prior to a vector consisting of zeros with dimensions (1,2*latent_features)\n",
    "        # self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        \n",
    "        '''\n",
    "        According to page 31-32 the iVAE consist of 7 NNs:\n",
    "        1. lambdaf prior\n",
    "        \n",
    "        2. X-encoder (Classic image CNN)\n",
    "        3. (Y, E)-encoder\n",
    "        4. (X, Y, E)-merger/encoder\n",
    "\n",
    "        5. Decoder\n",
    "\n",
    "        1: Learn priors based on the label distribution for the given environment\n",
    "        2-4: Encoding X, encoding Y and E and merging these two encoders, to generate a \n",
    "             qz which is conditional on the environment.\n",
    "        5: Decodes the latent space through pz. Since the latent space now contain some measure\n",
    "           of environment, then this distribution pz is consequentially conditioned on the environment\n",
    "\n",
    "        NN 1-3 can be found in the variational inference funktion.\n",
    "        '''\n",
    "        #### PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS ####\n",
    "        #NN 1/5\n",
    "        self.Lambdaf_prior = nn.Sequential(\n",
    "            nn.Linear(in_features = 2, out_features=50), #Input\n",
    "            nn.Linear(in_features = 50, out_features=50), #Fully connected\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 50, out_features = 20))  #Output\n",
    "        #### PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS ####\n",
    "\n",
    "         #For NN 2/5 X-Encoder: Inference Network\n",
    "         #Encode the observation `x` into the parameters of the posterior distribution\n",
    "         #`q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoderCNN1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.encoderCNN2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.encoderCNN3 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        ##NN 3/5 (Y, E)-Encoder\n",
    "        self.YEencoder = nn.Sequential(\n",
    "            nn.Linear(in_features = 2, out_features=100),\n",
    "            nn.Linear(in_features = 100, out_features = 100),\n",
    "            nn.ReLU())\n",
    "\n",
    "        ##NN 4/5 (X, Y, E)-merger/encoder\n",
    "        #remember to concatenate x.flatten, y, e before running this.\n",
    "        self.XYEmerger = nn.Sequential(\n",
    "            nn.Linear(in_features = 32 * 4 * 4 + 100, out_features=100),\n",
    "            nn.Linear(in_features = 100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 100, out_features = 2*latent_features))\n",
    "\n",
    "\n",
    "        #For NN 5/5 (Decoder): Generative Model\n",
    "        #Decode the latent sample `z` into the parameters of the observation model\n",
    "        #`p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoderFFN = nn.Linear(in_features=latent_features, out_features = 32 * 4 * 4)\n",
    "        self.decoderFFN2 = nn.Linear(in_features = 32 * 4 * 4, out_features = 32 * 4 * 4)\n",
    "        \n",
    "        self.decoderCNN1 = nn.ConvTranspose2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 0)\n",
    "        self.decoderCNN2 = nn.ConvTranspose2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 1)\n",
    "        self.decoderCNN3 = nn.ConvTranspose2d(in_channels = 32, out_channels = 3, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 1)\n",
    "\n",
    "    def prior_params(self, y, e):\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        ye = torch.cat((y, e), dim = 1)\n",
    "        ye = ye.to(torch.float32)\n",
    "        lambdaf_parameters = self.Lambdaf_prior(ye)\n",
    "\n",
    "        return lambdaf_parameters\n",
    "\n",
    "    #NN 4/7\n",
    "    def encoder(self, x, y, e):\n",
    "        x = self.relu(self.encoderCNN1(x))\n",
    "        x = self.relu(self.encoderCNN2(x))\n",
    "        x = self.relu(self.encoderCNN3(x))\n",
    "        x = x.view(x.size(0), -1) #NN 4/7\n",
    "\n",
    "        ye = torch.cat((y, e), dim = 1)\n",
    "        ye = ye.to(torch.float32)\n",
    "        ye = self.YEencoder(ye) #NN 5/7\n",
    "\n",
    "        xye = torch.cat((x,ye), dim = 1)\n",
    "        xye = self.XYEmerger(xye) #NN 6/7\n",
    "    \n",
    "        return xye\n",
    "   \n",
    "    #NN 7/7\n",
    "    def decoder(self, z):\n",
    "        \n",
    "        x = self.relu(self.decoderFFN(z))\n",
    "        x = self.relu(self.decoderFFN2(x))\n",
    "        \n",
    "        # reshape x and add CNN decoder\n",
    "        x = x.view(-1, 32, 4, 4)\n",
    "        \n",
    "        x = self.relu(self.decoderCNN1(x))\n",
    "        x = self.relu(self.decoderCNN2(x))\n",
    "        x = self.decoderCNN3(x)\n",
    "        return x    \n",
    "        \n",
    "    def posterior(self, x:Tensor, y:Tensor, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x, y, z)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def prior(self, y, e)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        #Expand prior_params til at være samme antal rækker som i den valgte batch size således at der fås\n",
    "        #en tensor med dimensionerne (batch_size, 2*latent_features), som så kan udfyldes med\n",
    "        prior_params = self.prior_params(y, e)\n",
    "        #chunk opdeler prior_params i to dele, de første 0-latent_features kollonner indeholder mu og \n",
    "        #de sidste n_latent_features inde holder sigmaerne. Nu er der to tensors, som begge har dim\n",
    "        #(batch_size, n_latent_features). Værdierne i disse tensors, kan bruges til at sample hvordan\n",
    "        #latent space ser ud (Den der hedder 'latent interpolations' i plots i bunden.)\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        #BEMÆRK at at det er log_sigma, dvs. at når den inputtes i ReparameterizedDiagonalGaussian så fås mu = 0, sigma = 1\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_means = self.decoder(z)\n",
    "        px_means = px_means.view(-1, *self.input_shape) # reshape the output #old\n",
    "        log_var = 0.0025 * torch.ones(px_means.shape)\n",
    "        log_sigma = torch.log(torch.sqrt(log_var))\n",
    "        log_sigma = log_sigma.to(self.device)\n",
    "        #sandsynlighedsfordeling der giver 1 eller 0, baseret på log-odds givet i logits input fra p(x|z).\n",
    "        #Dvs. at px_logits angiver sandsynligheden for at det givne pixel er henholdsvist rød,grøn,blå. Pixel værdien\n",
    "        #er enten 0 eller 1. Når man sampler fra bernoulli fordelingen fås dermed et billede, som givet z, giver en figur,\n",
    "        #som er bestemt af de sandsynligheder der er i px_logits (p(x|z)). Dvs. at for et givet latents space, kan en\n",
    "        #figur/et tal reproduceres ud fra de beregnede sandsynligheder og den efterfølgende sample fra Bernoulli fordelingen.\n",
    "        return ReparameterizedDiagonalGaussian(mu = px_means, log_sigma = log_sigma)\n",
    "        \n",
    "\n",
    "    def forward(self, x, y, e) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        ###############################################################################################\n",
    "        # flatten the input\n",
    "        #x = x.view(x.size(0), -1) #outcommented as part of adding CNN\n",
    "        \n",
    "        #### Run through ENCODER and calculate mu and sigma for latent space sampling\n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x, y, e)\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        #### LATENT SPACE\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        #### DECODER\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        ###############################################################################################\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        #(Indgår i beregning af kl-term (regularisering) ifm. ELBO) - og bruges også til interpolations visualisering\n",
    "        #til sidst.\n",
    "        pz = self.prior(y,e)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "    \n",
    "    \n",
    "    def sample_from_prior(self, y, e):\n",
    "        \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "        \n",
    "        # Laver bare reconstruction baseret på latent space\n",
    "        #Kan evt. fjernes. Anvendes bare til at vise hvor god modellen er til at generere data baseret på\n",
    "        #latent space genererede data. Funktionen anvendes kun i make_vae_plots.\n",
    "        \n",
    "        # degine the prior p(z)\n",
    "        pz = self.prior(y, e)\n",
    "        \n",
    "        # sample the prior \n",
    "        z = pz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'z': z}\n",
    "    \n",
    "    def reduce(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "        return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "    def VariationalInference(self, x, y, e, beta):\n",
    "        self.beta = beta\n",
    "        # forward pass through the model\n",
    "        outputs = self.forward(x, y, e)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = self.reduce(px.log_prob(x)) #log(p(x|z)): Sandsynligheden for at observere vores input variabel x\n",
    "        #givet vores latent space (tjekker modellens evne til at rekonstruere sig selv, ved at maximere sandsynlig-\n",
    "        #heden for at observere inputtet selv, givet det konstruerede latent space.\n",
    "        log_pz = self.reduce(pz.log_prob(z)) #log(p(z)): Sandsynligheden for at observere vores latent space, givet at\n",
    "        #latent space følger en standard-normal fordeling (Jo højere sandsynlighed jo bedre)\n",
    "        log_qz = self.reduce(qz.log_prob(z)) #log(q(z|x)): Sandsynligheden for at generere netop dette latent space givet \n",
    "        #vores input billede x. Denne værdi skal helst være lav?\n",
    "        \n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        #########################################################################################################\n",
    "        # Reconstruction loss: E_q [ log p(x|z) ]\n",
    "        # Regularization term: \\beta * D_KL(q(z|x) | p(z))` => Forsøger at tvinge fordelingen q(z|x) mod N(0,1)?\n",
    "        #########################################################################################################\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl\n",
    "        beta_elbo = log_px - self.beta * kl\n",
    "        \n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "latent_features = 10 #Husk at opdater denne parameter nede i 'initialization', hvis den skal bruges i VAE loopet også\n",
    "ivae = iVariationalAutoencoder(images[0].shape, latent_features, device)\n",
    "ivae = ivae.to(device)\n",
    "print(ivae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivae.load_state_dict(torch.load('100epochs_iVAE_var005_flipped.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy code for the testing of the PC algorithm to ensure the model data is prepared in the right format\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cdt\n",
    "cdt.SETTINGS.rpath = 'C:/Program Files/R/R-4.2.2/bin/Rscript' # this path should point to your own R implementation !\n",
    "from cdt.causality.graph import PC\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "#cmnist = ConcatDataset([cmnist_data1, cmnist_data2])\n",
    "#loader = DataLoader(cmnist, batch_size = 40000, batch_sampler = None)\n",
    "\n",
    "sample = next(iter(loader))\n",
    "\n",
    "x = sample['image']\n",
    "y = sample['label'].reshape(-1,1)\n",
    "e = sample['environment'].reshape(-1,1)\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "e = e.to(device)\n",
    "\n",
    "ivae = ivae.to(device)\n",
    "\n",
    "output = ivae.forward(x, y, e)\n",
    "\n",
    "z = output['z']\n",
    "\n",
    "z_full = z\n",
    "x_full = x\n",
    "y_full = y\n",
    "e_full = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qz = output['qz']\n",
    "z_sample1 = qz.rsample()\n",
    "z_sample2 = qz.rsample()\n",
    "z_sample3 = qz.rsample()\n",
    "z_sample4 = qz.rsample()\n",
    "z_sample5 = qz.rsample()\n",
    "z_sample6 = qz.rsample()\n",
    "z_sample7 = qz.rsample()\n",
    "z_sample8 = qz.rsample()\n",
    "z_sample9 = qz.rsample()\n",
    "z_sample10 = qz.rsample()\n",
    "\n",
    "y = y.detach()\n",
    "e = e.detach()\n",
    "\n",
    "\n",
    "\n",
    "y_df = pd.DataFrame(torch.cat((y,y,y,y,y,y,y,y,y,y), dim = 0))\n",
    "z_df = pd.DataFrame(torch.cat((z_sample1.detach(),z_sample2.detach(),z_sample3.detach(),z_sample4.detach(),z_sample5.detach(),\n",
    "    z_sample6.detach(),z_sample7.detach(),z_sample8.detach(),z_sample9.detach(),z_sample10.detach(),), dim = 0))\n",
    "df = y_df#; df = z_df; #df = z_df;  \n",
    "df['Z1'] = z_df[0]\n",
    "df['Z2'] = z_df[1]\n",
    "df['Z3'] = z_df[2]\n",
    "df['Z4'] = z_df[3]\n",
    "df['Z5'] = z_df[4]\n",
    "df['Z6'] = z_df[5]\n",
    "df['Z7'] = z_df[6]\n",
    "df['Z8'] = z_df[7]\n",
    "df['Z9'] = z_df[8]\n",
    "df['Z10'] = z_df[9]\n",
    "\n",
    "\n",
    "df.columns = ['Y','Z1', 'Z2', 'Z3', 'Z4', 'Z5', 'Z6', 'Z7', 'Z8', 'Z9', 'Z10']\n",
    "df.head()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parents(pc_array,y_pos,z_pos):\n",
    "    ind = np.where(pc_array[z_pos,y_pos] == 1)\n",
    "    ind_DAGpar = []\n",
    "    ind_par = []\n",
    "    for i in ind[0]:\n",
    "        if pc_array[y_pos,i] == 0:\n",
    "            ind_DAGpar.append(i)\n",
    "        else:\n",
    "            ind_par.append(i)\n",
    "    return ind_DAGpar, ind_par\n",
    "\n",
    "pc = PC(alpha=0.001)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 16, node_size = 9000)\n",
    "plt.savefig('PC_iVAE_alpha_005.png', transparent=True)\n",
    "plt.show()\n",
    "\n",
    "pc_array = pc._run_pc(df)\n",
    "parents = torch.tensor(find_parents(pc_array, 0, range(0,10))[0]).to(device)\n",
    "parents = parents - 1\n",
    "print(parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found indices:\n",
    "\n",
    "#tensor([0, 3, 4, 6]) kører, og har alpha = 0.05 - Skal testes, men umiddelbart dårligt\n",
    "#tensor([2, 5]) kører, cherry-picked. Har dårlig performance på iVAE\n",
    "#tensor([3, 4, 6]) \n",
    "#parents = parents.to(\"cpu\")\n",
    "parents = torch.tensor([2,7])\n",
    "options = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "children = np.delete(options, parents)\n",
    "nParents = len(parents)\n",
    "nChildren = 10 - nParents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy function from week 4:\n",
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "#Building a classifier:\n",
    "\n",
    "class ClassifieriVAE(nn.Module):\n",
    "    def __init__(self, indices):\n",
    "        super().__init__()\n",
    "        self.indices = indices\n",
    "        #Layers\n",
    "        self.input_layer = nn.Linear(in_features=2, out_features = 50)\n",
    "        self.layer1 = nn.Linear(in_features = 50, out_features = 100)\n",
    "        self.output_layer = nn.Linear(in_features = 100, out_features = 1)\n",
    "\n",
    "        #activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "    #forward function(self, x):\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.sigmoid(self.output_layer(x))\n",
    "\n",
    "        return x\n",
    "    # \n",
    "\n",
    "classifieriVAE = ClassifieriVAE(parents)\n",
    "classifieriVAE = classifieriVAE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn import metrics\n",
    "\n",
    "#Define loss function\n",
    "loss_fn = nn.BCELoss() \n",
    "#The adam algorithm automatically use momentum\n",
    "optimizer = optim.Adam(classifieriVAE.parameters(), lr = 1e-4) \n",
    "\n",
    "#Test model\n",
    "#z = output['z']\n",
    "#z = torch.index_select(z, 1, parents)\n",
    "#out = classifier(z[1])\n",
    "#print(\"Output shape:\", out.size())\n",
    "#print(f\"Output logits:\\n{out.detach().cpu().numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "ivae = ivae.to(device)\n",
    "z = z.to(device)\n",
    "parents = parents.to(device)\n",
    "classifieriVAE = classifieriVAE.to(device)\n",
    "\n",
    "#Set iVAE to eval - the no.grad thing is not an issue as we are only using the encoder.\n",
    "ivae.eval()\n",
    "\n",
    "#TRAINING THE CLASSIFIER\n",
    "epoch = 0\n",
    "num_epochs = 10 #hyper parametre\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "#device = torch.device(\"cpu\")\n",
    "\n",
    "# training..\n",
    "training_epoch_data = []\n",
    "test_epoch_data = []\n",
    "test_acc, train_acc = [], []\n",
    "\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    training_batch_data = []\n",
    "    test_batch_data = []\n",
    "    classifieriVAE.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for sample in train_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label'].to(torch.float32)\n",
    "        y = y.to(device)\n",
    "\n",
    "        e = sample['environment'].to(torch.float32)\n",
    "        e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = ivae.posterior(x, y.reshape(-1,1), e.reshape(-1,1))\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        output = classifieriVAE(z).view(-1)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current batch\n",
    "        training_batch_data.append(loss)\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    training_epoch_data.append(torch.tensor(training_batch_data).mean())\n",
    "    print(\"Epoch: {}, Mean epoch loss: {}\".format(epoch, torch.tensor(training_batch_data).mean()))\n",
    "    #print(\"Epoch {} : , Loss {}\".format(epoch, training_epoch_data[epoch-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NF_ivae = NF_ivae.to(device)\n",
    "#indices = indices.to(device)\n",
    "#parents = parents.to(device)\n",
    "\n",
    "#Set iVAE to eval - the no.grad thing is not an issue as we are only using the encoder.\n",
    "#NF_ivae.eval()\n",
    "\n",
    "#TRAINING THE CLASSIFIER\n",
    "epoch = 0\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\">> Using device: {device}\")\n",
    "\n",
    "# training..\n",
    "training_epoch_data = []\n",
    "test_epoch_data = []\n",
    "test_acc, train_acc = [], []\n",
    "\n",
    "\n",
    "#Evaluate training\n",
    "with torch.no_grad():\n",
    "    classifieriVAE.eval()\n",
    "    train_targs, train_preds = [], []\n",
    "    for sample in train_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label']\n",
    "        y = y.to(device)\n",
    "\n",
    "        e = sample['environment']\n",
    "        e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = ivae.posterior(x, y.reshape(-1,1), e.reshape(-1,1))\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        output = classifieriVAE(z).view(-1)\n",
    "        prediction = torch.round(output)\n",
    "\n",
    "        train_targs += list(y.cpu().numpy())\n",
    "        train_preds += list(prediction.data.cpu().numpy())\n",
    "\n",
    "    #Evaluating validation\n",
    "    val_targs, val_preds = [], []\n",
    "    for sample in test_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label']\n",
    "        y = y.to(device)\n",
    "\n",
    "        e = sample['environment']\n",
    "        e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = ivae.posterior(x, y.reshape(-1,1), e.reshape(-1,1))\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        output = classifieriVAE(z).view(-1)\n",
    "        prediction = torch.round(output)\n",
    "\n",
    "\n",
    "        val_targs += list(y.cpu().numpy())\n",
    "        val_preds += list(prediction.data.cpu().numpy())\n",
    "\n",
    "train_acc_cur = accuracy(Tensor(train_targs),Tensor(train_preds))\n",
    "train_acc.append(train_acc_cur)\n",
    "\n",
    "test_acc_cur = accuracy(Tensor(val_targs),Tensor(val_preds))\n",
    "test_acc.append(test_acc_cur)\n",
    "print(\"Epoch %2i : , Train acc %f, Valid acc %f\" % (\n",
    "            epoch, train_acc_cur, test_acc_cur))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(classifieriVAE.state_dict(), 'classifieriVAEv1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([np.array(val_targs).mean(),np.array(val_preds).mean()]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Performance compared to iCaRL in the paper (table on page. 27): {}%\".format(round(correct/(correct+false) * 100 / 68.75 * 100,2)))\n",
    "\n",
    "print(\"Performance: {}%\".format(round(correct/(correct+false) * 100,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NF-iVAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Bernoulli, Normal\n",
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class NF_iVAE(nn.Module):\n",
    "    \"\"\"A Variational Autoencoder with\n",
    "    * a Bernoulli observation model `p_\\theta(x | z) = B(x | g_\\theta(z))`\n",
    "    * a Gaussian prior `p(z) = N(z | 0, I)`\n",
    "    * a Gaussian posterior `q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_shape:torch.Size, latent_features:int, device) -> None:\n",
    "        super(NF_iVAE, self).__init__()\n",
    "        \n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "\n",
    "        self.device = device#torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = torch.device(\"cpu\")\n",
    "        # define the parameters of the prior, chosen as p(z) = N(0, I)\n",
    "        ## setting the prior to a vector consisting of zeros with dimensions (1,2*latent_features)\n",
    "        # self.register_buffer('prior_params', torch.zeros(torch.Size([1, 2*latent_features])))\n",
    "        \n",
    "        '''\n",
    "        According to page 31-32 the iVAE consist of 7 NNs:\n",
    "        1. TNN prior\n",
    "        2. lambdaNN prior\n",
    "        3. lambdaf prior\n",
    "        \n",
    "        4. X-encoder (Classic image CNN)\n",
    "        5. (Y, E)-encoder\n",
    "        6. (X, Y, E)-merger/encoder\n",
    "\n",
    "        7. Decoder\n",
    "\n",
    "        1-3: Learn priors based on the label distribution for the given environment\n",
    "        4-6: Encoding X, encoding Y and E and merging these two encoders, to generate a \n",
    "             qz which is conditional on the environment.\n",
    "        7: Decodes the latent space through pz. Since the latent space now contain some measure\n",
    "           of environment, then this distribution pz is consequentially conditioned on the environment\n",
    "\n",
    "        NN 1-3 can be found in the variational inference funktion.\n",
    "        '''\n",
    "        #### PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS ####\n",
    "        #NN 1/7\n",
    "        self.TNN_prior = nn.Sequential(\n",
    "            nn.Linear(in_features = latent_features, out_features=50), #Input\n",
    "            nn.Linear(in_features = 50, out_features=50), #Fully connected\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 50, out_features = 45)) #Output\n",
    "\n",
    "        #NN 2/7\n",
    "        self.LambdaNN_prior = nn.Sequential(\n",
    "            nn.Linear(in_features = 2, out_features=50), #Input\n",
    "            nn.Linear(in_features = 50, out_features=50), #Fully connected\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 50, out_features = 45)) #Output\n",
    "\n",
    "        #NN 3/7\n",
    "        self.Lambdaf_prior = nn.Sequential(\n",
    "            nn.Linear(in_features = 2, out_features=50), #Input\n",
    "            nn.Linear(in_features = 50, out_features=50), #Fully connected\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 50, out_features = 20))  #Output\n",
    "        #### PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS PRIORS ####\n",
    "\n",
    "         #For NN 4/7 X-Encoder: Inference Network\n",
    "         #Encode the observation `x` into the parameters of the posterior distribution\n",
    "         #`q_\\phi(z|x) = N(z | \\mu(x), \\sigma(x)), \\mu(x),\\log\\sigma(x) = h_\\phi(x)`\n",
    "        self.encoderCNN1 = nn.Conv2d(in_channels = 3, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.encoderCNN2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        self.encoderCNN3 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        ##NN 5/7 (Y, E)-Encoder\n",
    "        self.YEencoder = nn.Sequential(\n",
    "            nn.Linear(in_features = 2, out_features=100),\n",
    "            nn.Linear(in_features = 100, out_features = 100),\n",
    "            nn.ReLU())\n",
    "\n",
    "        ##NN 6/7 (X, Y, E)-merger/encoder\n",
    "        #remember to concatenate x.flatten, y, e before running this.\n",
    "        self.XYEmerger = nn.Sequential(\n",
    "            nn.Linear(in_features = 32 * 4 * 4 + 100, out_features=100),\n",
    "            nn.Linear(in_features = 100, out_features = 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features = 100, out_features = 2*latent_features))\n",
    "\n",
    "\n",
    "        #For NN 7/7 (Decoder): Generative Model\n",
    "        #Decode the latent sample `z` into the parameters of the observation model\n",
    "        #`p_\\theta(x | z) = \\prod_i B(x_i | g_\\theta(x))`\n",
    "        self.decoderFFN = nn.Linear(in_features=latent_features, out_features = 32 * 4 * 4)\n",
    "        self.decoderFFN2 = nn.Linear(in_features = 32 * 4 * 4, out_features = 32 * 4 * 4)\n",
    "        \n",
    "        self.decoderCNN1 = nn.ConvTranspose2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 0)\n",
    "        self.decoderCNN2 = nn.ConvTranspose2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 1)\n",
    "        self.decoderCNN3 = nn.ConvTranspose2d(in_channels = 32, out_channels = 3, kernel_size = 3, stride = 2, padding = 1\n",
    "                              ,output_padding = 1)\n",
    "\n",
    "    #Non-factorized prior for NF-iVAE from page 28\n",
    "    def non_factorized_prior_pz(self, z, TNN_parameters, lambdaNN_parameters, lambdaf_parameters, reduce = True):\n",
    "        #beregning fra side 28 under M.2. P_T,lambda(Z|Y,E).\n",
    "        z_cat = torch.cat((z, z.pow(2)), dim = 1) \n",
    "        # \"(*).sum(dim=1)\" is the vector-vector dot product. It is not possible to make this calculation with e.g. torch.tensordot, as we want the dot product of each vector and not the entire batch\n",
    "        non_factorized_prior = (TNN_parameters*lambdaNN_parameters).sum(dim = 1) + (z_cat * lambdaf_parameters).sum(dim = 1)\n",
    "        #the f,z term - (z_cat * lambdaf_parameters).sum(dim = 1): Is equivalent to the factorized exponential family\n",
    "        #the NN term - (TNN_parameters*lambdaNN_parameters).sum(dim = 1): Capture the dependencies between the latent variables\n",
    "        return non_factorized_prior\n",
    "\n",
    "    def parameters_for_non_factorized_prior(self, z, y, e):\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        TNN_parameters = self.TNN_prior(z)\n",
    "        ye = torch.cat((y, e), dim = 1)\n",
    "        ye = ye.to(torch.float32)\n",
    "        LambdaNN_parameters = self.LambdaNN_prior(ye)\n",
    "        lambdaf_parameters = self.Lambdaf_prior(ye)\n",
    "\n",
    "        return TNN_parameters, LambdaNN_parameters, lambdaf_parameters\n",
    "\n",
    "    #NN 4/7\n",
    "    def encoder(self, x, y, e):\n",
    "        x = self.relu(self.encoderCNN1(x))\n",
    "        x = self.relu(self.encoderCNN2(x))\n",
    "        x = self.relu(self.encoderCNN3(x))\n",
    "        x = x.view(x.size(0), -1) #NN 4/7\n",
    "\n",
    "        ye = torch.cat((y, e), dim = 1)\n",
    "        ye = ye.to(torch.float32)\n",
    "        ye = self.YEencoder(ye) #NN 5/7\n",
    "\n",
    "        xye = torch.cat((x,ye), dim = 1)\n",
    "        xye = self.XYEmerger(xye) #NN 6/7\n",
    "    \n",
    "        return xye\n",
    "   \n",
    "    #NN 7/7\n",
    "    def decoder(self, z):\n",
    "        \n",
    "        x = self.relu(self.decoderFFN(z))\n",
    "        x = self.relu(self.decoderFFN2(x))\n",
    "        \n",
    "        # reshape x and add CNN decoder\n",
    "        x = x.view(-1, 32, 4, 4)\n",
    "        \n",
    "        x = self.relu(self.decoderCNN1(x))\n",
    "        x = self.relu(self.decoderCNN2(x))\n",
    "        x = self.decoderCNN3(x)\n",
    "        return x\n",
    "        \n",
    "    def posterior(self, x:Tensor, y:Tensor, e:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "        \n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.encoder(x, y, e)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        px_loc = self.decoder(z)\n",
    "        px_loc = px_loc.view(-1, *self.input_shape) # reshape the output #old\n",
    "        shape = torch.ones(px_loc.shape)\n",
    "        shape = shape.to(self.device)\n",
    "        shape = 0.05 * shape \n",
    "        #sandsynlighedsfordeling der giver 1 eller 0, baseret på log-odds givet i logits input fra p(x|z).\n",
    "        #Dvs. at px_logits angiver sandsynligheden for at det givne pixel er henholdsvist rød,grøn,blå. Pixel værdien\n",
    "        #er enten 0 eller 1. Når man sampler fra bernoulli fordelingen fås dermed et billede, som givet z, giver en figur,\n",
    "        #som er bestemt af de sandsynligheder der er i px_logits (p(x|z)). Dvs. at for et givet latents space, kan en\n",
    "        #figur/et tal reproduceres ud fra de beregnede sandsynligheder og den efterfølgende sample fra Bernoulli fordelingen.\n",
    "        #return Bernoulli(logits=px_loc, validate_args=False)\n",
    "        return Normal(loc=px_loc, scale = shape)\n",
    "        \n",
    "\n",
    "    def forward(self, x, y, e) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        #### Run through ENCODER and calculate mu and sigma for latent space sampling\n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x, y, e)\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        #### LATENT SPACE\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        #### DECODER\n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "\n",
    "        return {'px': px, 'qz': qz, 'z': z}\n",
    "\n",
    "    def reduce(self, x:Tensor) -> Tensor:\n",
    "        \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "        return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "    def VariationalInference(self, x, y, e):\n",
    "        \n",
    "        # forward pass through the model to get the encoder and decoder outputs\n",
    "        parameters_and_latent_space = self.forward(x, y, e)\n",
    "\n",
    "        # unpack encoder parameters from (px), decoder parameters (qz) and the latent space (z)\n",
    "        px_ze, qz_xye, z = [parameters_and_latent_space[k] for k in [\"px\", \"qz\", \"z\"]]\n",
    "\n",
    "\n",
    "        # DEFINE THE PRIOR p(z)\n",
    "        #### PRIOR\n",
    "        z_temp = z.detach().requires_grad_(requires_grad = True)\n",
    "        y_temp = y.detach()\n",
    "        e_temp = e.detach()\n",
    "        TNN_parameters, LambdaNN_parameters, lambdaf_parameters = self.parameters_for_non_factorized_prior(z_temp, y_temp, e_temp) #bruger temp da priors træning ikke skal påvirke encoder og decoder\n",
    "\n",
    "        #prior calculation from page 28 (til differentiering ifm. eq. 64 skal dette ikke være logget, men når\n",
    "        # elbo loss beregnes, skal man huske at tage log at denne værdi jvf. p. 28)\n",
    "\n",
    "        #According to page 6. equation (8)-(10)\n",
    "        TNN_parameters_hat = TNN_parameters.detach()\n",
    "        LambdaNN_parameters_hat = LambdaNN_parameters.detach()\n",
    "        lambdaf_parameters_hat = lambdaf_parameters.detach()\n",
    "\n",
    "        log_pz_ye_ELBO =  self.non_factorized_prior_pz(z, TNN_parameters_hat, LambdaNN_parameters_hat, lambdaf_parameters_hat)\n",
    "        log_pz_ye_SM = self.non_factorized_prior_pz(z_temp, TNN_parameters, LambdaNN_parameters, lambdaf_parameters) #phi_hat (or phi.detach()) from page. 6 is implemented implicitly/auto-\n",
    "        #matically through the implementation of SM on page 28 eq. (64), where autograd of p_T,lambda is used instead of the conditional distribution q_phi(Z|X,Y,E), here phi\n",
    "        #is evidently excluded from the gradient calculation.\n",
    "        #OBS: HER SKAL DER IKKE ANVENDES LOG-PROBS AF PZ_YE JVF. SM DELEN AF EQ. (64) PÅ SIDE 28... eller hvad???\n",
    "        dpdz_ye = torch.autograd.grad(log_pz_ye_SM.sum(), z_temp, create_graph = True, retain_graph=True)[0]\n",
    "\n",
    "        #ddpz_ye = torch.autograd.grad(dpz_ye.mean(), z_temp, create_graph = True, retain_graph=True)[0] #changed sum() to mean()\n",
    "        ddpdz_sq_ye = torch.autograd.grad(dpdz_ye.sum(), z_temp, create_graph = True, retain_graph=True)[0] #original\n",
    "\n",
    "        #### SM loss SM loss SM loss SM loss SM loss SM loss SM loss SM loss ####\n",
    "        #Calculation from page 28 equation 64\n",
    "        SM = (ddpdz_sq_ye + 0.5 * dpdz_ye.pow(2)).sum(1)\n",
    "        #### SM loss SM loss SM loss SM loss SM loss SM loss SM loss SM loss ####\n",
    "\n",
    "        #### ELBO loss ELBO loss ELBO loss ELBO loss ELBO loss ELBO loss ELBO loss ####\n",
    "        # evaluate log probabilities\n",
    "        # Skal jvf. s. 32 ændres til at være en normal fordeling i stedet for en\n",
    "        # bernoulli fordeling, og her skal mean være outputtet af decoderen og varians skal\n",
    "        #blot sættes til at være 0.01\n",
    "        \n",
    "        log_px_ze = self.reduce(px_ze.log_prob(x)) #log(p(x|z)): Sandsynligheden for at observere vores input variabel x\n",
    "        #givet vores latent space (tjekker modellens evne til at rekonstruere sig selv, ved at maximere sandsynlig-\n",
    "        #heden for at observere inputtet selv, givet det konstruerede latent space.\n",
    "        \n",
    "        ####(old)log_pz = reduce(pz.log_prob(z)) #log(p(z)): \n",
    "        #log_pz_ye = torch.log(pz_ye)#Sandsynligheden for at observere vores latent space, givet at\n",
    "        #latent space følger en standard-normal fordeling (Jo højere sandsynlighed jo bedre)\n",
    "        \n",
    "        log_qz_xye = self.reduce(qz_xye.log_prob(z)) #log(q(z|x)): Sandsynligheden for at generere netop dette latent space givet \n",
    "        #vores input billede x. Denne værdi skal helst være lav?\n",
    "        \n",
    "        # compute the ELBO: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        #########################################################################################################\n",
    "        # Reconstruction loss: E_q [ log p(x|z) ]\n",
    "        # Regularization term: \\beta * D_KL(q(z|x) | p(z))` => Forsøger at tvinge fordelingen q(z|x) mod N(0,1)?\n",
    "        #########################################################################################################\n",
    "        \n",
    "        kl = log_qz_xye - log_pz_ye_ELBO\n",
    "        \n",
    "        elbo = log_px_ze - kl\n",
    "        ####\n",
    "        \n",
    "        # loss\n",
    "        loss = -(elbo.mean() - SM.mean())\n",
    "        #print(\"SM: {}. elbo: {} = log_px_ze: {} + log_pz_ye: {} - log_qz_xye: {}\".format(SM.mean(), elbo.mean(),log_px_ze.mean(),pz_ye.mean(),log_qz_xye.mean()))\n",
    "\n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px_ze, 'kl': kl}\n",
    "        \n",
    "        outputs = parameters_and_latent_space\n",
    "        return loss, diagnostics, outputs\n",
    "\n",
    "    #def sample_from_prior(self, batch_size:int=100):\n",
    "    #    \"\"\"sample z~p(z) and return p(x|z)\"\"\"\n",
    "    #    \n",
    "    #   # Laver bare reconstruction baseret på latent space\n",
    "    #    #Kan evt. fjernes. Anvendes bare til at vise hvor god modellen er til at generere data baseret på\n",
    "    #    #latent space genererede data. Funktionen anvendes kun i make_vae_plots.\n",
    "    #    \n",
    "    #    # degine the prior p(z)\n",
    "    #    pz = self.prior(batch_size=batch_size)\n",
    "    #    \n",
    "    #    # sample the prior \n",
    "    #    z = pz.rsample()\n",
    "    #    \n",
    "    #    # define the observation model p(x|z) = B(x | g(z))\n",
    "    #    px = self.observation_model(z)\n",
    "    #    \n",
    "    #    return {'px': px, 'pz': pz, 'z': z}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "latent_features = 10 #Husk at opdater denne parameter nede i 'initialization', hvis den skal bruges i VAE loopet også\n",
    "batch_size = 256\n",
    "NF_ivae = NF_iVAE(images[0].shape, latent_features, device)\n",
    "NF_ivae = NF_ivae.to(device)\n",
    "print(NF_iVAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# VAE\n",
    "latent_features = 10 #Hyper parameter\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(NF_ivae.parameters(), lr=1e-4) #Hyper parameter, tilføj evt. weight_decay (L2 regularization)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)\n",
    "\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "num_epochs = 100 #hyper parametre\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "# move the model to the device \n",
    "sample_counter = 0\n",
    "# training..\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    NF_ivae.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for sample in tqdm(train_loader): #tqdm\n",
    "        sample_counter += 1\n",
    "        \n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label'].reshape(-1,1)\n",
    "        y = y.to(device)\n",
    "\n",
    "        e = sample['environment'].reshape(-1,1)\n",
    "        e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        optimizer.zero_grad()\n",
    "        loss, diagnostics, outputs = NF_ivae.VariationalInference(x, y, e)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    px = outputs['px']\n",
    "    samples = px.sample()\n",
    "\n",
    "    xplot = x[0].to(torch.device(\"cpu\"))\n",
    "    samplesplot = samples[0].to(torch.device(\"cpu\"))\n",
    "\n",
    "    #fig, axes = plt.subplots(2,2)\n",
    "    #show_CMNIST(xplot, label=int(y[0]), environment=int(e[0]), ax = axes[0,0])\n",
    "    #show_CMNIST(samplesplot, label=int(y[0]), environment=int(e[0]),  ax = axes[0,1])\n",
    "    #axes[1,0].plot(training_data['elbo'], label = \"elbo\")\n",
    "    #display(fig)\n",
    "    #clear_output(wait = True)\n",
    "\n",
    "\n",
    "    #if (epoch+1)%(num_epochs*0.1) == 0:\n",
    "        #print(\"epoch: {}: training loss: elbo = {}, kl = {}, log_px = {}, total loss = {}\".format(epoch, training_data['elbo'][epoch-1], training_data['kl'][epoch-1], training_data['log_px'][epoch-1]), loss)\n",
    "    \n",
    "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
    "    #make_vae_plots(ivae, x, y, outputs, training_data, validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(NF_ivae.state_dict(), '100epochs_NF_iVAE_var005_flipped_v3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_ivae.load_state_dict(torch.load('100epochs_NF_iVAE_var005_flipped_v3.pt', map_location=torch.device(device)))\n",
    "NF_ivae = NF_ivae.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dummy code for the testing of the PC algorithm to ensure the model data is prepared in the right format\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cdt\n",
    "cdt.SETTINGS.rpath = 'C:/Program Files/R/R-4.2.2/bin/Rscript' # this path should point to your own R implementation !\n",
    "from cdt.causality.graph import PC\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "#cmnist = ConcatDataset([cmnist_data1, cmnist_data2])\n",
    "#loader = DataLoader(cmnist, batch_size = 40000, batch_sampler = None)\n",
    "\n",
    "sample = next(iter(loader))\n",
    "\n",
    "x = sample['image']\n",
    "y = sample['label'].reshape(-1,1)\n",
    "e = sample['environment'].reshape(-1,1)\n",
    "\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "e = e.to(device)\n",
    "\n",
    "NF_ivae = NF_ivae.to(device)\n",
    "\n",
    "output = NF_ivae.forward(x, y, e)\n",
    "\n",
    "z = output['z']\n",
    "\n",
    "z_full = z\n",
    "x_full = x\n",
    "y_full = y\n",
    "e_full = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qz = output['qz']\n",
    "z_sample1 = qz.rsample()\n",
    "z_sample2 = qz.rsample()\n",
    "z_sample3 = qz.rsample()\n",
    "z_sample4 = qz.rsample()\n",
    "z_sample5 = qz.rsample()\n",
    "z_sample6 = qz.rsample()\n",
    "z_sample7 = qz.rsample()\n",
    "z_sample8 = qz.rsample()\n",
    "z_sample9 = qz.rsample()\n",
    "z_sample10 = qz.rsample()\n",
    "\n",
    "y = y.detach()\n",
    "e = e.detach()\n",
    "\n",
    "\n",
    "\n",
    "y_df = pd.DataFrame(torch.cat((y,y,y,y,y,y,y,y,y,y), dim = 0))\n",
    "z_df = pd.DataFrame(torch.cat((z_sample1.detach(),z_sample2.detach(),z_sample3.detach(),z_sample4.detach(),z_sample5.detach(),\n",
    "    z_sample6.detach(),z_sample7.detach(),z_sample8.detach(),z_sample9.detach(),z_sample10.detach(),), dim = 0))\n",
    "df = y_df#; df = z_df; #df = z_df;  \n",
    "df['Z1'] = z_df[0]\n",
    "df['Z2'] = z_df[1]\n",
    "df['Z3'] = z_df[2]\n",
    "df['Z4'] = z_df[3]\n",
    "df['Z5'] = z_df[4]\n",
    "df['Z6'] = z_df[5]\n",
    "df['Z7'] = z_df[6]\n",
    "df['Z8'] = z_df[7]\n",
    "df['Z9'] = z_df[8]\n",
    "df['Z10'] = z_df[9]\n",
    "\n",
    "\n",
    "df.columns = ['Y','Z1', 'Z2', 'Z3', 'Z4', 'Z5', 'Z6', 'Z7', 'Z8', 'Z9', 'Z10']\n",
    "df.head()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glasso = cdt.independence.graph.Glasso()\n",
    "\n",
    "skeleton = glasso.predict(df)\n",
    "\n",
    "#model_pc = cdt.causality.graph.PC(alpha = 0.2)\n",
    "\n",
    "from causallearn.search.ConstraintBased.PC  import pc\n",
    "\n",
    "from causallearn.utils.cit import fisherz\n",
    "cg = pc(np.array(df),  0.05, fisherz, True, 0, 0)\n",
    "\n",
    "cg.draw_pydot_graph()\n",
    "#graph_pc = model_pc.predict(df, skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_parents(pc_array,y_pos,z_pos):\n",
    "    ind = np.where(pc_array[z_pos,y_pos] == 1)\n",
    "    ind_DAGpar = []\n",
    "    ind_par = []\n",
    "    for i in ind[0]:\n",
    "        if pc_array[y_pos,i] == 0:\n",
    "            ind_DAGpar.append(i)\n",
    "        else:\n",
    "            ind_par.append(i)\n",
    "    return ind_DAGpar, ind_par\n",
    "\n",
    "pc = PC(alpha=0.05)\n",
    "pc_output = pc.predict(df)\n",
    "\n",
    "nx.draw(pc_output, with_labels=True, font_weight='bold', font_size = 16, node_size = 9000)\n",
    "plt.savefig('PC_iVAE_alpha_005.png', transparent=True)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_array = pc._run_pc(df)\n",
    "parents = torch.tensor(find_parents(pc_array, 0, range(0,10))[0]).to(device)\n",
    "parents = parents - 1\n",
    "print(parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Found indices:\n",
    " \n",
    "#parents = parents.to(\"cpu\")\n",
    "parents = torch.tensor([1,2])\n",
    "options = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "children = np.delete(options, parents)\n",
    "nParents = len(parents)\n",
    "nChildren = 10 - nParents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy function from week 4:\n",
    "def accuracy(target, pred):\n",
    "    return metrics.accuracy_score(target.detach().cpu().numpy(), pred.detach().cpu().numpy())\n",
    "\n",
    "#Building a classifier:\n",
    "\n",
    "class ClassifierNF_iVAE(nn.Module):\n",
    "    def __init__(self, indices):\n",
    "        super().__init__()\n",
    "        self.indices = indices\n",
    "        #Layers\n",
    "        self.input_layer = nn.Linear(in_features=2, out_features = 50)\n",
    "        self.layer1 = nn.Linear(in_features = 50, out_features = 100)\n",
    "        self.output_layer = nn.Linear(in_features = 100, out_features = 1)\n",
    "\n",
    "        #activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "    #forward function(self, x):\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input_layer(x)\n",
    "        x = self.relu(self.layer1(x))\n",
    "        x = self.sigmoid(self.output_layer(x))\n",
    "\n",
    "        return x\n",
    "    # \n",
    "\n",
    "classifierNF_iVAE = ClassifierNF_iVAE(parents)\n",
    "classifierNF_iVAE = classifierNF_iVAE.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NF_ivae = NF_ivae.to(device)\n",
    "#indices = indices.to(device)\n",
    "parents = parents.to(device)\n",
    "classifierNF_iVAE = classifierNF_iVAE.to(device)\n",
    "\n",
    "#Set iVAE to eval - the no.grad thing is not an issue as we are only using the encoder.\n",
    "NF_ivae.eval()\n",
    "\n",
    "#TRAINING THE CLASSIFIER\n",
    "epoch = 0\n",
    "num_epochs = 10 #hyper parametre\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\">> Using device: {device}\")\n",
    "\n",
    "# training..\n",
    "training_epoch_data = []\n",
    "test_epoch_data = []\n",
    "test_acc, train_acc = [], []\n",
    "\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "    training_batch_data = []\n",
    "    test_batch_data = []\n",
    "    classifierNF_iVAE.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for sample in train_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label'].to(torch.float32)\n",
    "        y = y.to(device)\n",
    "\n",
    "        e = sample['environment'].to(torch.float32)\n",
    "        e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = NF_ivae.posterior(x, y.reshape(-1,1), e.reshape(-1,1))\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        output = classifierNF_iVAE(z).view(-1)\n",
    "\n",
    "        loss = loss_fn(output, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current batch\n",
    "        training_batch_data.append(loss)\n",
    "            \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    training_epoch_data.append(torch.tensor(training_batch_data).mean())\n",
    "    print(\"Epoch: {}, Mean epoch loss: {}\".format(epoch, torch.tensor(training_batch_data).mean()))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NF_ivae = NF_ivae.to(device)\n",
    "#indices = indices.to(device)\n",
    "#parents = parents.to(device)\n",
    "\n",
    "#Set iVAE to eval - the no.grad thing is not an issue as we are only using the encoder.\n",
    "#NF_ivae.eval()\n",
    "\n",
    "#TRAINING THE CLASSIFIER\n",
    "epoch = 0\n",
    "#batch size hyper parameter can be changed in the dataloader in the beginning.\n",
    "\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f\">> Using device: {device}\")\n",
    "\n",
    "# training..\n",
    "training_epoch_data = []\n",
    "test_epoch_data = []\n",
    "test_acc, train_acc = [], []\n",
    "\n",
    "\n",
    "#Evaluate training\n",
    "with torch.no_grad():\n",
    "    classifierNF_iVAE.eval()\n",
    "    train_targs, train_preds = [], []\n",
    "    for sample in train_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label']\n",
    "        y = y.to(device)\n",
    "\n",
    "        e = sample['environment']\n",
    "        e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = NF_ivae.posterior(x, y.reshape(-1,1), e.reshape(-1,1))\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        output = classifierNF_iVAE(z).view(-1)\n",
    "        prediction = torch.round(output)\n",
    "\n",
    "        train_targs += list(y.cpu().numpy())\n",
    "        train_preds += list(prediction.data.cpu().numpy())\n",
    "\n",
    "    #Evaluating validation\n",
    "    val_targs, val_preds = [], []\n",
    "    for sample in test_loader:\n",
    "        x = sample['image']\n",
    "        x = x.to(device)\n",
    "\n",
    "        y = sample['label']\n",
    "        y = y.to(device)\n",
    "\n",
    "        e = sample['environment']\n",
    "        e = e.to(device)\n",
    "        \n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        qz = NF_ivae.posterior(x, y.reshape(-1,1), e.reshape(-1,1))\n",
    "        z = qz.rsample()\n",
    "        z = torch.index_select(z, 1, parents)\n",
    "\n",
    "        #Get classification prediction\n",
    "        output = classifierNF_iVAE(z).view(-1)\n",
    "        prediction = torch.round(output)\n",
    "\n",
    "\n",
    "        val_targs += list(y.cpu().numpy())\n",
    "        val_preds += list(prediction.data.cpu().numpy())\n",
    "\n",
    "train_acc_cur = accuracy(Tensor(train_targs),Tensor(train_preds))\n",
    "train_acc.append(train_acc_cur)\n",
    "\n",
    "test_acc_cur = accuracy(Tensor(val_targs),Tensor(val_preds))\n",
    "test_acc.append(test_acc_cur)\n",
    "print(\"Epoch %2i : , Train acc %f, Valid acc %f\" % (\n",
    "            epoch, train_acc_cur, test_acc_cur))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([np.array(y),np.array(prediction)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(prediction == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(prediction == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "test_data_full = DataLoader(cmnist_test_data, batch_size = 20000, batch_sampler = None)\n",
    "\n",
    "sample = next(iter(test_data_full))\n",
    "\n",
    "x_test = sample['image']\n",
    "y_test = sample['label'].reshape(-1,1)\n",
    "e_test = sample['environment'].reshape(-1,1)\n",
    "\n",
    "x_test = x_test.to(device)\n",
    "y_test = y_test.to(device)\n",
    "e_test = e_test.to(device)\n",
    "\n",
    "NF_ivae = NF_ivae.to(device)\n",
    "output = NF_ivae.forward(x_test, y_test, e_test)\n",
    "\n",
    "classifierNF_iVAE = classifierNF_iVAE.to(device)\n",
    "\n",
    "z_test = output['z']\n",
    "\n",
    "z_full = z_test.detach()\n",
    "x_full = x_test.detach()\n",
    "y_full = y_test.detach()\n",
    "e_full = e_test.detach()\n",
    "\n",
    "Zp = torch.randn(z_full.shape[0], nParents)\n",
    "Zc = torch.randn(z_full.shape[0], nChildren)\n",
    "Z = torch.randn(z_full.shape)\n",
    "\n",
    "\n",
    "\n",
    "Zp = Zp.to(device)\n",
    "Zc = Zc.to(device)\n",
    "Z = Z.to(device)\n",
    "\n",
    "loss_BCE = nn.BCELoss()\n",
    "\n",
    "Zp.requires_grad = True\n",
    "Zc.requires_grad = True\n",
    "\n",
    "optimizer = torch.optim.Adam(params=[Zp, Zc], lr=1e-1)\n",
    "\n",
    "def get_reconstruction(Zp, Zc):\n",
    "    Z[:,parents] = Zp\n",
    "    Z[:,children] = Zc\n",
    "    return(NF_ivae.observation_model(Z))\n",
    "\n",
    "loss_values = []\n",
    "\n",
    "\n",
    "for i in tqdm(range(num_epochs)):\n",
    "    sample = next(iter(test_data_full))\n",
    "    x = sample['image']\n",
    "    x = x.to(device)\n",
    "\n",
    "    px = get_reconstruction(Zp,Zc)\n",
    "    loss = -px.log_prob(x)\n",
    "    loss = loss.mean()\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)\n",
    "    optimizer.step()\n",
    "\n",
    "    loss_values.append(loss.detach().cpu())\n",
    "    #print(Z[0,:])\n",
    "    #print(\"Step done\")\n",
    "    #print(loss)\n",
    "\n",
    "plt.plot(loss_values)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "3edaea292c0121a4edf57fb9b1fd8f07f262e12aa8a67f7e3de01891f262632f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
